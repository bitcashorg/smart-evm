{
  "relatedBlogs": [
    {
      "id": "190259319",
      "topics": [
        "Summary",
        "LLM",
        "Training"
      ],
      "title": "Latent Space Podcast 8/16/23 [Summary] - The Mathematics of Training LLMs — with Quentin Anthony of Eleuther AI",
      "slug": "latent-space-podcast-8-16-23-summary-the-mathematics-of-training-llms-with-que",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T09:19:45+01:00",
      "description": "Explore the math behind training LLMs with Quentin Anthony from Eleuther AI. Dive into the Transformers Math 101 article & master distributed training techniques for peak GPU performance.",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692324088-screenshot-2023-08-17-at-9-59-17-pm.png"
      },
      "seo": {
        "description": "Dive into the Transformers Math 101 article & master distributed training techniques for peak GPU performance.",
        "title": "Latent Space Podcast 8/16/23 [Summary] Math of Training LLMs",
        "twitterCard": null,
        "image": {
          "width": 1576,
          "height": 554,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1692324088-screenshot-2023-08-17-at-9-59-17-pm.png"
        }
      }
    },
    {
      "id": "190259129",
      "topics": [
        "LLM",
        "Hardware",
        "Summary",
        "Edge"
      ],
      "title": "Latent Space Podcast 8/10/23 [Summary]: LLMs Everywhere: Running 70B models in browsers and iPhones using MLC — with Tianqi Chen of CMU / OctoML",
      "slug": "latent-space-podcast-8-10-23-summary-llms-everywhere-running-70b-models-in-browse",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T09:18:37+01:00",
      "description": "Explore the magic of MLC with Tianqi Chen: deploying 70B models on browsers & iPhones. Dive into XGBoost, TVM's creation, & the future of universal AI deployments. ",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1691894611-screenshot-2023-08-12-at-10-42-43-pm.png"
      },
      "seo": {
        "description": "Explore deploying 70B models on browsers & iPhones. Dive into XGBoost, TVM's creation, & the future of universal AI deployments. ",
        "title": "Latent Space 8/10/23 [Summary]: LLMs Everywhere",
        "twitterCard": null,
        "image": {
          "width": 1538,
          "height": 548,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1691894611-screenshot-2023-08-12-at-10-42-43-pm.png"
        }
      }
    },
    {
      "id": "190259087",
      "topics": [
        "Summary",
        "LLM",
        "Code",
        "Open Source",
        "Small Models"
      ],
      "title": "Latent Space Podcast 8/4/23 [Summary] Latent Space x AI Breakdown crossover pod! ",
      "slug": "latent-space-podcast-8-4-23-summary-latent-space-x-ai-breakdown-crossover-pod",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T09:16:33+01:00",
      "description": "Join AI Breakdown & Latent Space for the summer AI tech roundup: Dive into GPT4.5, Llama 2, AI tools, the rising AI engineer, and more!",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1691539617-screenshot-2023-08-08-at-8-02-52-pm.png"
      },
      "seo": {
        "description": "Dive into GPT4.5, Llama 2, AI tools, the rising AI engineer, and more!",
        "title": "Latent Space Podcast 8/4/23 [Summary] AI Breakdown crossover",
        "twitterCard": null,
        "image": {
          "width": 1578,
          "height": 558,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1691539617-screenshot-2023-08-08-at-8-02-52-pm.png"
        }
      }
    },
    {
      "id": "190259111",
      "topics": [
        "Summary",
        "Transformers",
        "Training",
        "Open Source"
      ],
      "title": " Latent Space Podcast 7/26/23 [Summary] FlashAttention 2: making Transformers 800% faster - Tri Dao of Together AI",
      "slug": "latent-space-podcast-7-26-23-summary-flashattention-2-making-transformers-800-fas",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T09:14:13+01:00",
      "description": "Discover how FlashAttention revolutionized AI speed with Tri Dao, as he unveils the power of FlashAttention 2, dives into Stanford's Hazy Lab & future AI insights.",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1691543194-screenshot-2023-08-08-at-8-43-59-pm.png"
      },
      "seo": {
        "description": "Discover how FlashAttention revolutionized AI speed with Tri Dao, as he unveils the power of FlashAttention 2",
        "title": " Latent Space Podcast 7/26/23 [Summary] FlashAttention 2",
        "twitterCard": null,
        "image": {
          "width": 1648,
          "height": 594,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1691543194-screenshot-2023-08-08-at-8-43-59-pm.png"
        }
      }
    },
    {
      "id": "190259191",
      "topics": [
        "Summary",
        "Code",
        "LLM"
      ],
      "title": "Latent Space Podcast 7/10/23 [Summary] - Code Interpreter == GPT 4.5 (w/ Simon Willison, Alex Volkov, Aravind Srinivas, Alex Graveley, et al.)",
      "slug": "latent-space-podcast-7-10-23-summary-code-interpreter-gpt-4-5-w-simon-willison-al",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T09:09:26+01:00",
      "description": "Explore ChatGPT's Code Interpreter: a game-changer in AI. Dive into its 1000x capabilities leap with Simon, Alex & top AI experts. #CodeAugmentedInference #GPT4_5",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692048911-screenshot-2023-08-14-at-3-34-05-pm.png"
      },
      "seo": {
        "description": "Explore ChatGPT's Code Interpreter: a game-changer in AI. Dive into its 1000x capabilities leap with Simon, Alex & top AI experts. ",
        "title": "Latent Space Podcast  [Summary] Code Interpreter = GPT 4.5",
        "twitterCard": null,
        "image": {
          "width": 1596,
          "height": 582,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1692048911-screenshot-2023-08-14-at-3-34-05-pm.png"
        }
      }
    },
    {
      "id": "190259216",
      "topics": [
        "Summary",
        "Open Source"
      ],
      "title": "Latent Space Podcast 7/2/23 [Summary] AI Trends: a Latent Space x Practical AI crossover pod!",
      "slug": "latent-space-podcast-7-2-23-summary-ai-trends-a-latent-space-x-practical-ai-cross",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T09:08:36+01:00",
      "description": "Explore the fusion of Practical AI & Latent Space as they delve into 2023's top AI trends, reflect on standout episodes, and share insights on navigating the AI evolution.",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692146916-screenshot-2023-08-15-at-5-20-38-pm.png"
      },
      "seo": {
        "description": "2023's top AI trends, reflect on standout episodes, and share insights on navigating the AI evolution.",
        "title": "Latent Space Podcast 7/2/23 [Summary] AI Trends ",
        "twitterCard": null,
        "image": {
          "width": 1600,
          "height": 532,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1692146916-screenshot-2023-08-15-at-5-20-38-pm.png"
        }
      }
    },
    {
      "id": "190259238",
      "topics": [
        "Hardware",
        "LLM",
        "Summary"
      ],
      "title": "Latent Space Podcast 6/20/23 [Summary] - Commoditizing the Petaflop — with George Hotz of the tiny corp",
      "slug": "latent-space-podcast-6-20-23-summary-commoditizing-the-petaflop-with-george-ho",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T09:07:15+01:00",
      "description": "George Hotz of tiny corp challenges Nvidia & Google! Dive into the world of AMD collaborations, insights on ggml, Mojo, Elon & GPT-4, plus a peek into AI Girlfriend. ",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692154615-screenshot-2023-08-15-at-10-55-40-pm.png"
      },
      "seo": {
        "description": "George Hotz of tiny corp challenges Nvidia & Google! AMD collaborations, insights on ggml, Mojo, Elon & GPT-4, plus a peek into AI Girlfriend. ",
        "title": "Latent Space Podcast 6/20/23 [Summary] - George Hotz ",
        "twitterCard": null,
        "image": {
          "width": 1586,
          "height": 508,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1692154615-screenshot-2023-08-15-at-10-55-40-pm.png"
        }
      }
    },
    {
      "id": "190259294",
      "topics": [
        "LLM",
        "Functions",
        "Summary"
      ],
      "title": "Latent Space Podcast 6/14/23 [Summary] - Emergency Pod: OpenAI's new Functions API, 75% Price Drop, 4x Context Length (w/ Alex Volkov, Simon Willison, Riley Goodside, Joshua Lochner, Stefania Druga, Eric Elliott, Mayo Oshin et al)",
      "slug": "latent-space-podcast-6-14-23-summary-emergency-pod-openai-s-new-functions-api-75",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T09:05:04+01:00",
      "description": "Explore the June 2023 OpenAI updates with top AI engineers from Scale, Microsoft, Pinecone, & Huggingface. Dive into the Code x LLM paradigms and discover Recursive Function Agents.",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692221668-screenshot-2023-08-16-at-5-32-29-pm.png"
      },
      "seo": {
        "description": "Explore the June 2023 OpenAI updates with top AI engineers from Scale, Microsoft, Pinecone, & Huggingface. ",
        "title": "Latent Space Podcast 6/20/23 [Summary] - Emergency Pod",
        "twitterCard": null,
        "image": {
          "width": 1626,
          "height": 606,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1692221668-screenshot-2023-08-16-at-5-32-29-pm.png"
        }
      }
    },
    {
      "id": "190259333",
      "topics": [
        "LLM",
        "Summary",
        "UX"
      ],
      "title": "Latent Space Podcast 6/8/23 [Summary] - From RLHF to RLHB: The Case for Learning from Human Behavior - with Jeffrey Wang and Joe Reeve of Amplitude",
      "slug": "latent-space-podcast-6-8-23-summary-from-rlhf-to-rlhb-the-case-for-learning-from",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T09:02:33+01:00",
      "description": "Explore AI & analytics with Jeffrey Wang & Joe Reeve on Latent Space Live! Dive into why AI values Analytics and the power of first-party behavioral data. ",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692386432-screenshot-2023-08-18-at-3-17-04-pm.png"
      },
      "seo": {
        "description": "Explore AI & analytics with Jeffrey Wang & Joe Reeve on Latent Space Live! Dive into why AI values Analytics and the power of first-party behavioral data. ",
        "title": "Latent Space Podcast 6/8/23 [Summary] - From RLHF to RLHB",
        "twitterCard": null,
        "image": {
          "width": 1674,
          "height": 550,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1692386432-screenshot-2023-08-18-at-3-17-04-pm.png"
        }
      }
    },
    {
      "id": "190260528",
      "topics": [
        "Summary",
        "LLM",
        "UX"
      ],
      "title": "Latent Space Podcast 6/1/23 [Summary] - Building the AI × UX Scenius — with Linus Lee of Notion AI",
      "slug": "latent-space-podcast-6-1-23-summary-building-the-ai-x-ux-scenius-with-linus-le",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T09:00:12+01:00",
      "description": "Explore Notion AI's transformative approach to AI and UX. Dive into the future of AI-augmented workspaces, the value beyond chat interfaces, and insights on effective knowledge work. Recap of AI×UX NYC meetup included!",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692390655-screenshot-2023-08-18-at-4-28-51-pm.png"
      },
      "seo": {
        "description": "Explore Notion AI's transformative approach to AI and UX. ",
        "title": "Latent Space Podcast 6/1/23 [Summary] - AI × UX Scenius",
        "twitterCard": null,
        "image": {
          "width": 1614,
          "height": 546,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1692390655-screenshot-2023-08-18-at-4-28-51-pm.png"
        }
      }
    },
    {
      "id": "190260557",
      "topics": [
        "Summary",
        "Code",
        "LLM",
        "Agents"
      ],
      "title": "Latent Space Podcast 5/25/23 [Summary] - Debugging the Internet with AI agents – with Itamar Friedman of Codium AI and AutoGPT",
      "slug": "latent-space-podcast-5-25-23-summary-debugging-the-internet-with-ai-agents-with",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T08:58:27+01:00",
      "description": "Explore the future of AI with Itamar Friedman from Codium AI on 'Debugging the Internet'. Dive into 'Extreme DRY' agents, the rapid sync of specs & tests, and the balance between code & testing. Plus, insights from Toran & an exclusive look at AutoGPT's roadmap!",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692397413-screenshot-2023-08-18-at-6-10-09-pm.png"
      },
      "seo": {
        "description": "Dive into 'Extreme DRY' agents, the rapid sync of specs & tests, and the balance between code & testing. ",
        "title": "Latent Space Pod 5/25/23 [Summary] Debugging the Internet",
        "twitterCard": null,
        "image": {
          "width": 1568,
          "height": 548,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1692397413-screenshot-2023-08-18-at-6-10-09-pm.png"
        }
      }
    },
    {
      "id": "190260577",
      "topics": [
        "LLM",
        "Small Models"
      ],
      "title": "Latent Space Podcast 5/20/23 [Summary] - MPT-7B and The Beginning of Context=Infinity — with Jonathan Frankle and Abhinav Venigalla of MosaicML",
      "slug": "latent-space-podcast-5-20-23-summary-mpt-7b-and-the-beginning-of-context-infinity",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T08:57:33+01:00",
      "description": "Dive into MosaicML's 9-day, $200k \"llongboi\" MPT-7B training, data prep insights, & the rise of open AI models with experts Frankle & Venigalla.",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692409795-screenshot-2023-08-18-at-9-49-21-pm.png"
      },
      "seo": {
        "description": "Dive into MosaicML's 9-day, $200k \"llongboi\" MPT-7B training, data prep insights, & the rise of open AI models ",
        "title": "Latent Space Podcast 6/25/23 [Summary] MosaicML",
        "twitterCard": null,
        "image": {
          "width": 1568,
          "height": 556,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1692409795-screenshot-2023-08-18-at-9-49-21-pm.png"
        }
      }
    },
    {
      "id": "190260597",
      "topics": [
        "LLM",
        "Structured Data"
      ],
      "title": "Latent Space Podcast 5/15/23 [Summary] - Guaranteed quality and structure in LLM outputs - with Shreya Rajpal of Guardrails AI",
      "slug": "latent-space-podcast-5-15-23-summary-guaranteed-quality-and-structure-in-llm-outp",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T08:56:26+01:00",
      "description": "Explore Ep. 12 with Shreya Rajpal of Guardrails AI: Dive deep into validating LLM outputs, refining answers through re-asking loops, and establishing SLAs for models. Master the nuances of AI quality assurance.",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692495732-screenshot-2023-08-19-at-9-38-27-pm.png"
      },
      "seo": {
        "description": "Explore Ep. 12 with Shreya Rajpal of Guardrails AI: Dive deep into validating LLM outputs.",
        "title": "Latent Space Podcast 5/15/23 [Summary] Quality LLM Outputs",
        "twitterCard": null,
        "image": {
          "width": 1580,
          "height": 512,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1692495732-screenshot-2023-08-19-at-9-38-27-pm.png"
        }
      }
    },
    {
      "id": "190260606",
      "topics": [
        "LLM",
        "Training",
        "Agents",
        "Multimodal"
      ],
      "title": "Latent Space Podcast 5/8/23 [Summary] - The AI Founder Gene: Being Early, Building Fast, and Believing in Greatness — with Sharif Shameem of Lexica",
      "slug": "latent-space-podcast-5-8-23-summary-the-ai-founder-gene-being-early-building-fast",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T08:52:09+01:00",
      "description": "Ep.11 with Sharif Shameem of Lexica: Dive into the AI founder mindset, uncovering the secrets to pioneering innovation, building game-changing tech, training models, and the intriguing potential of Agents and genomic sequencing. ",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692501984-screenshot-2023-08-19-at-11-24-05-pm.png"
      },
      "seo": {
        "description": "Ep.11 with Sharif Shameem of Lexica: Dive into the AI founder mindset, uncovering the secrets to pioneering innovation.",
        "title": "Latent Space Pod 5/8/23 [Summary] The AI Founder Gene",
        "twitterCard": null,
        "image": {
          "width": 1606,
          "height": 550,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1692501984-screenshot-2023-08-19-at-11-24-05-pm.png"
        }
      }
    },
    {
      "id": "190260640",
      "topics": [
        "Summary",
        "Open Source",
        "LLM"
      ],
      "title": "Latent Space Podcast 5/5/23 [Summary] - No Moat: Closed AI gets its Open Source wakeup call — ft. Simon Willison",
      "slug": "latent-space-podcast-5-5-23-summary-no-moat-closed-ai-gets-its-open-source-wakeup",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T08:49:18+01:00",
      "description": "Explore 'No Moat: Closed AI's Open Source Awakening' with Simon Willison. Dive into leaked Google Moat memo insights, Google Brain Drain, and Python's speed boost with Mojo.",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692566921-screenshot-2023-08-20-at-5-25-53-pm.png"
      },
      "seo": {
        "description": "Explore 'No Moat: Closed AI's Open Source Awakening' with Simon Willison. Dive into leaked Google Moat memo insights.",
        "title": "Latent Space Podcast 5/5/23 [Summary] - No Moat",
        "twitterCard": null,
        "image": {
          "width": 1602,
          "height": 532,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1692566921-screenshot-2023-08-20-at-5-25-53-pm.png"
        }
      }
    },
    {
      "id": "190260671",
      "topics": [
        "LLM",
        "Code",
        "Summary"
      ],
      "title": "Latent Space Podcast 5/3/23 [Summary] - Training a SOTA Code LLM in 1 week and Quantifying the Vibes — with Reza Shabani of Replit",
      "slug": "latent-space-podcast-5-3-23-summary-training-a-sota-code-llm-in-1-week-and-quanti",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T08:46:08+01:00",
      "description": "Ep. 10 with Reza Shabani: Dive deep into the rapid training of a state-of-the-art Code LLM, explore Replit Ghostwriter's future, and journey from Finance to AI. Discover the transition from Kaplan to Chinchilla and more!",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692584998-screenshot-2023-08-20-at-10-17-26-pm.png"
      },
      "seo": {
        "description": "Ep. 10 with Reza Shabani: Dive deep into the rapid training of a state-of-the-art Code LLM!",
        "title": "Latent Space Pod 5/3/23 [Summary] - SOTA Code LLM",
        "twitterCard": null,
        "image": {
          "width": 1566,
          "height": 530,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1692584998-screenshot-2023-08-20-at-10-17-26-pm.png"
        }
      }
    },
    {
      "id": "190629271",
      "topics": [
        "LLM",
        "Small Models",
        "Summary"
      ],
      "title": "Latent Space Podcast 4/28/23 [Summary] - Mapping the future of *truly* Open Models and Training Dolly for $30 — with Mike Conover of Databricks",
      "slug": "latent-space-podcast-4-28-23-summary-mapping-the-future-of-truly-open-models-and",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T08:33:12+01:00",
      "description": "Explore the future of open models with Mike Conover of Databricks. Dive deep into Dolly's creation, its transition from 1.0 to 2.0, & the influences behind its development. Ep.9 touches on model infrastructure, Databricks' vision, & more. #AI #OpenModels #Dolly",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1694038707-screenshot-2023-09-06-at-3-12-24-pm.png"
      },
      "seo": {
        "description": "Ep.9 touches on model infrastructure, Databricks' vision, & more. #AI #OpenModels #Dolly",
        "title": "Latent Space Pod 4/28/23 [Summary] - Mike of Databricks",
        "twitterCard": null,
        "image": {
          "width": 1572,
          "height": 628,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1694038707-screenshot-2023-09-06-at-3-12-24-pm.png"
        }
      }
    },
    {
      "id": "191164291",
      "topics": [
        "LLM",
        "Enterprise",
        "Summary"
      ],
      "title": "Latent Space Podcast 4/21/23 [Summary] - AI-powered Search for the Enterprise — with Deedy Das of Glean",
      "slug": "latent-space-podcast-4-21-23-summary-ai-powered-search-for-the-enterprise-with",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T08:31:31+01:00",
      "description": "Ep.8: Dive into AI in enterprise search with Deedy Das of Glean. Unpack challenges in creating an AI search giant, Google vs ChatGPT comparisons, AI infrastructure intricacies, spotting AI-generated text, and why businesses need more than just Document QA.",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1694134074-screenshot-2023-09-07-at-5-43-48-pm.png"
      },
      "seo": {
        "description": "Ep.8: Dive into AI in enterprise search with Deedy Das of Glean. Unpack challenges in creating an AI search giant, Google vs ChatGPT ...",
        "title": "Latent Space Podcast 4/21/23 [Summary] - with Deedy Das ",
        "twitterCard": null,
        "image": {
          "width": 1608,
          "height": 530,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1694134074-screenshot-2023-09-07-at-5-43-48-pm.png"
        }
      }
    },
    {
      "id": "191165673",
      "topics": [
        "Summary",
        "Vision"
      ],
      "title": "Latent Space Podcast 4/13/23 [Summary] - Segment Anything Model and the Hard Problems of Computer Vision — with Joseph Nelson of Roboflow",
      "slug": "latent-space-podcast-4-13-23-summary-segment-anything-model-and-the-hard-problems",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T08:30:03+01:00",
      "description": "Explore Ep.7 with Joseph Nelson on the Segment Anything Model by Meta. Dive deep into Computer Vision's future, the significance of OCR, Image Segmentation, and beyond. #Roboflow #AI",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1694150379-screenshot-2023-09-07-at-10-15-52-pm.png"
      },
      "seo": {
        "description": "Dive deep into Computer Vision's future, the significance of OCR, Image Segmentation, and beyond. #Roboflow #AI",
        "title": "Latent Space Podcast 4/13/23 [Summary] - Segment Anything ",
        "twitterCard": null,
        "image": {
          "width": 1604,
          "height": 604,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1694150379-screenshot-2023-09-07-at-10-15-52-pm.png"
        }
      }
    }
  ],
  "blogContent": {
    "id": "190259172",
    "topics": [
      "Summary",
      "LLM",
      "Open Source",
      "Small Models"
    ],
    "title": "Latent Space Podcast 7/19/23 [Summary] - Llama 2: The New Open LLM SOTA (ft. Nathan Lambert, Matt Bornstein, Anton Troynikov, Russell Kaplan, Whole Mars Catalog et al.)",
    "slug": "latent-space-podcast-7-19-23-summary-llama-2-the-new-open-llm-sota-ft-nathan-lamb",
    "authorName": "Prof. Otto Nomos",
    "authorPicture": {
      "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
    },
    "_publishedAt": "2023-10-05T09:12:38+01:00",
    "description": "Explore Llama 2, the latest AI breakthrough with experts Nathan Lambert, Matt Bornstein & more. Dive into datasets, benchmarks & AI predictions. Llama insights & drama await in this top podcast!",
    "thumbnail": {
      "url": "https://www.datocms-assets.com/101962/1691968295-screenshot-2023-08-13-at-7-11-06-pm.png"
    },
    "contentBlock": [
      {
        "mainContent": {
          "value": {
            "schema": "dast",
            "document": {
              "type": "root",
              "children": [
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Original Link: Llama 2: "
                    },
                    {
                      "url": "https://www.latent.space/p/llama2#details",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "The New Open LLM SOTA (ft. Nathan Lambert, Matt Bornstein, Anton Troynikov, Russell Kaplan, Whole Mars Catalog et al.)"
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "Summary"
                    }
                  ]
                },
                {
                  "type": "heading",
                  "level": 3,
                  "children": [
                    {
                      "type": "span",
                      "value": "Introduction"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "\nIn an episode of a podcast focusing on AI research and models, Alessio Fanelli hosts with guests Simon Willison and Nathan Lambert."
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Alessio Fanelli"
                            },
                            {
                              "type": "span",
                              "value": " reiterates the podcast's ongoing commitment to discussing AI topics, hinting at its increasing depth. He acknowledges Simon as a frequent guest and thanks Nathan, who has shared insights on the technical details of Lama two."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Simon Willison"
                            },
                            {
                              "type": "span",
                              "value": " expresses his excitement about the release of LAMA two. He highlights that this version can be used for commercial purposes, marking a significant change from previous versions which were not. He also mentions that while the benchmarks suggest it's a quality product, time will be required to ascertain its real-world effectiveness. The model, although officially available through Meta's website upon form approval, has already seen unofficial distributions online."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Nathan Lambert"
                            },
                            {
                              "type": "span",
                              "value": ", who is affiliated with Huggingface, shares his experience on the topic. He's a researcher involved in reinforcement learning from human feedback. He emphasizes the significance of LAMA two in terms of its research contribution. However, Nathan also points out that the paper's methodology is more evident than the specifics of the data sets used. This shift may be related to potential legal challenges regarding the training data used in the first LAMA. In the previous model, copyrighted data was involved, which may be one reason behind the decreased transparency in the new paper."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Matt Bornstein"
                            },
                            {
                              "type": "span",
                              "value": " from a16z briefly mentions their version of evaluations, positioning it as more of a \"feel-good\" approach in contrast to the deep dives of other panelists."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "The conversation revolves around the implications of LAMA two, its commercial viability, technical intricacies, and potential legal and ethical ramifications."
                    }
                  ]
                }
              ]
            }
          }
        },
        "topImages": [
          {
            "basename": "screenshot-2023-08-13-at-7-11-06-pm",
            "height": 574,
            "width": 1632,
            "filename": "screenshot-2023-08-13-at-7-11-06-pm.png",
            "format": "png",
            "alt": null,
            "url": "https://www.datocms-assets.com/101962/1691968295-screenshot-2023-08-13-at-7-11-06-pm.png"
          }
        ]
      },
      {
        "mainContent": {
          "value": {
            "schema": "dast",
            "document": {
              "type": "root",
              "children": [
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "LAMA 2 Model Insights and Open Source Licensing Discussion"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "marks": [
                        "strong"
                      ],
                      "value": "\nHighlights:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "numbered",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Diverse Model Sizes:"
                            },
                            {
                              "type": "span",
                              "value": " Alessio Fanelli highlights various sizes of the LAMA 2 models, including 7 billion, 13 billion, and 70 billion, but points out a 34 billion model size that was deemed \"unsafe\" and was not released due to safety concerns and time constraints."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Data Opacity:"
                            },
                            {
                              "type": "span",
                              "value": " There is a significant increase in pre-training corpus size for LAMA 2 (40% larger), but specific sources remain undisclosed, unlike LAMA 1."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Safety Overkill?"
                            },
                            {
                              "type": "span",
                              "value": " There are concerns about LAMA 2's overly cautious safety responses to certain queries, prompting discussions on the balance between safety and practicality."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Licensing and Open Source:"
                            },
                            {
                              "type": "span",
                              "value": " A major topic of discussion revolves around the term \"open source\" and its implications. The distinction between \"openly licensed\" and \"open source\" is emphasized, along with the complications introduced by terms like \"competing large language model.\""
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Commercial Potential:"
                            },
                            {
                              "type": "span",
                              "value": " There's strong anticipation of extensive commercial use of open source language models, but licensing questions, especially related to commercial applications, remain a constant challenge."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Model Potential and Optimizations:"
                            },
                            {
                              "type": "span",
                              "value": " With LAMA 2 being open source, there's excitement about potential optimizations and improvements from the broader AI research community."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Huggingface's Rail License:"
                            },
                            {
                              "type": "span",
                              "value": " Nathan Lambert from Huggingface discusses their Responsible AI License (RAIL) designed to be commercially available with good intentions and provides leeway to address bad actors using their models."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                }
              ]
            }
          }
        },
        "topImages": [
          {
            "basename": "abls4-img1",
            "height": 936,
            "width": 936,
            "filename": "abls4-img1.png",
            "format": "png",
            "alt": null,
            "url": "https://www.datocms-assets.com/101962/1691982762-abls4-img1.png"
          }
        ]
      },
      {
        "mainContent": {
          "value": {
            "schema": "dast",
            "document": {
              "type": "root",
              "children": [
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "Exploring the Evolution and Potential of Llama 2 in AI Pre-training"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Swyx"
                            },
                            {
                              "type": "span",
                              "value": " introduces the topic of pre-training base model discussions."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Nathan Lambert"
                            },
                            {
                              "type": "span",
                              "value": ":"
                            }
                          ]
                        },
                        {
                          "type": "list",
                          "style": "bulleted",
                          "children": [
                            {
                              "type": "listItem",
                              "children": [
                                {
                                  "type": "paragraph",
                                  "children": [
                                    {
                                      "type": "span",
                                      "value": "Mentions the role of cross-query attention (cqa) in making inference faster."
                                    }
                                  ]
                                }
                              ]
                            },
                            {
                              "type": "listItem",
                              "children": [
                                {
                                  "type": "paragraph",
                                  "children": [
                                    {
                                      "type": "span",
                                      "value": "Notes a lack of emphasis on code and math in the paper, despite its market importance."
                                    }
                                  ]
                                }
                              ]
                            },
                            {
                              "type": "listItem",
                              "children": [
                                {
                                  "type": "paragraph",
                                  "children": [
                                    {
                                      "type": "span",
                                      "value": "Appreciates the detailed information on RHF and sees it as confirmation of capabilities hinted at by companies like Anthropic and OpenAI."
                                    }
                                  ]
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Matt Bornstein"
                            },
                            {
                              "type": "span",
                              "value": ":"
                            }
                          ]
                        },
                        {
                          "type": "list",
                          "style": "bulleted",
                          "children": [
                            {
                              "type": "listItem",
                              "children": [
                                {
                                  "type": "paragraph",
                                  "children": [
                                    {
                                      "type": "span",
                                      "value": "Poses a question about pre-training data surpassing Chinchilla optimal."
                                    }
                                  ]
                                }
                              ]
                            },
                            {
                              "type": "listItem",
                              "children": [
                                {
                                  "type": "paragraph",
                                  "children": [
                                    {
                                      "type": "span",
                                      "value": "Reflects on the initial excitement when Chinchilla's findings showed improved performance with increased training data."
                                    }
                                  ]
                                }
                              ]
                            },
                            {
                              "type": "listItem",
                              "children": [
                                {
                                  "type": "paragraph",
                                  "children": [
                                    {
                                      "type": "span",
                                      "value": "Asks Nathan for clarification on how better data quality impacts performance."
                                    }
                                  ]
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Nathan Lambert"
                            },
                            {
                              "type": "span",
                              "value": ":"
                            }
                          ]
                        },
                        {
                          "type": "list",
                          "style": "bulleted",
                          "children": [
                            {
                              "type": "listItem",
                              "children": [
                                {
                                  "type": "paragraph",
                                  "children": [
                                    {
                                      "type": "span",
                                      "value": "Comments on the changing nature of data quality, highlighting issues like insider jokes, phrasings, and errors."
                                    }
                                  ]
                                }
                              ]
                            },
                            {
                              "type": "listItem",
                              "children": [
                                {
                                  "type": "paragraph",
                                  "children": [
                                    {
                                      "type": "span",
                                      "value": "Emphasizes the importance of deduplication and the challenges of determining good text from the internet."
                                    }
                                  ]
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Matt Bornstein"
                            },
                            {
                              "type": "span",
                              "value": ":"
                            }
                          ]
                        },
                        {
                          "type": "list",
                          "style": "bulleted",
                          "children": [
                            {
                              "type": "listItem",
                              "children": [
                                {
                                  "type": "paragraph",
                                  "children": [
                                    {
                                      "type": "span",
                                      "value": "Points out that early data limitations were possibly due to inadequate cleaning methods."
                                    }
                                  ]
                                }
                              ]
                            },
                            {
                              "type": "listItem",
                              "children": [
                                {
                                  "type": "paragraph",
                                  "children": [
                                    {
                                      "type": "span",
                                      "value": "Questions whether there's an upper limit to the amount of data we can train models on."
                                    }
                                  ]
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Swyx"
                            },
                            {
                              "type": "span",
                              "value": ":"
                            }
                          ]
                        },
                        {
                          "type": "list",
                          "style": "bulleted",
                          "children": [
                            {
                              "type": "listItem",
                              "children": [
                                {
                                  "type": "paragraph",
                                  "children": [
                                    {
                                      "type": "span",
                                      "value": "Comments on the ever-increasing ratios of tokens to parameters."
                                    }
                                  ]
                                }
                              ]
                            },
                            {
                              "type": "listItem",
                              "children": [
                                {
                                  "type": "paragraph",
                                  "children": [
                                    {
                                      "type": "span",
                                      "value": "Highlights how Chinchilla's methods were specific to their time and objectives and contrasts this with the new objectives presented in the Llama paper."
                                    }
                                  ]
                                }
                              ]
                            },
                            {
                              "type": "listItem",
                              "children": [
                                {
                                  "type": "paragraph",
                                  "children": [
                                    {
                                      "type": "span",
                                      "value": "Points out the early stage of AI model development and the rapid progression of research."
                                    }
                                  ]
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Simon Willison"
                            },
                            {
                              "type": "span",
                              "value": ":"
                            }
                          ]
                        },
                        {
                          "type": "list",
                          "style": "bulleted",
                          "children": [
                            {
                              "type": "listItem",
                              "children": [
                                {
                                  "type": "paragraph",
                                  "children": [
                                    {
                                      "type": "span",
                                      "value": "Observes that the rush to publish might stem from the competition in AI research."
                                    }
                                  ]
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Swyx"
                            },
                            {
                              "type": "span",
                              "value": ":"
                            }
                          ]
                        },
                        {
                          "type": "list",
                          "style": "bulleted",
                          "children": [
                            {
                              "type": "listItem",
                              "children": [
                                {
                                  "type": "paragraph",
                                  "children": [
                                    {
                                      "type": "span",
                                      "value": "Mentions the practice of shipping state-of-the-art models quickly."
                                    }
                                  ]
                                }
                              ]
                            },
                            {
                              "type": "listItem",
                              "children": [
                                {
                                  "type": "paragraph",
                                  "children": [
                                    {
                                      "type": "span",
                                      "value": "Points out that not everyone involved in Llama 2's development was credited in the paper due to employment transitions."
                                    }
                                  ]
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Alessio Fanelli"
                            },
                            {
                              "type": "span",
                              "value": ":"
                            }
                          ]
                        },
                        {
                          "type": "list",
                          "style": "bulleted",
                          "children": [
                            {
                              "type": "listItem",
                              "children": [
                                {
                                  "type": "paragraph",
                                  "children": [
                                    {
                                      "type": "span",
                                      "value": "Cautions against delving into organizational drama without firsthand knowledge."
                                    }
                                  ]
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Matt Bornstein"
                            },
                            {
                              "type": "span",
                              "value": " and "
                            },
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Nathan Lambert"
                            },
                            {
                              "type": "span",
                              "value": ":"
                            }
                          ]
                        },
                        {
                          "type": "list",
                          "style": "bulleted",
                          "children": [
                            {
                              "type": "listItem",
                              "children": [
                                {
                                  "type": "paragraph",
                                  "children": [
                                    {
                                      "type": "span",
                                      "value": "Express light-hearted surprise about the potential drama among researchers."
                                    }
                                  ]
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Swyx"
                            },
                            {
                              "type": "span",
                              "value": ":"
                            }
                          ]
                        },
                        {
                          "type": "list",
                          "style": "bulleted",
                          "children": [
                            {
                              "type": "listItem",
                              "children": [
                                {
                                  "type": "paragraph",
                                  "children": [
                                    {
                                      "type": "span",
                                      "value": "Moves the discussion towards preference data mentioned by Nathan."
                                    }
                                  ]
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "marks": [
                        "strong"
                      ],
                      "value": "Key Takeaways:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "value": "There's a consensus that the field of AI research is rapidly evolving."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "value": "The quality of data and how it's processed is paramount to improving model performance."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "value": "The competitive nature of AI research might lead to publishing models before their full potential is realized."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "value": "Some contributors to Llama 2 might not have been credited due to employment shifts, hinting at potential drama in the AI research community."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                }
              ]
            }
          }
        },
        "topImages": [
          {
            "basename": "abls4-img2",
            "height": 936,
            "width": 936,
            "filename": "abls4-img2.png",
            "format": "png",
            "alt": null,
            "url": "https://www.datocms-assets.com/101962/1691982774-abls4-img2.png"
          }
        ]
      },
      {
        "mainContent": {
          "value": {
            "schema": "dast",
            "document": {
              "type": "root",
              "children": [
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "Discussion on Meta's LAMA 2 Model and its Implications"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "marks": [
                        "strong"
                      ],
                      "value": "Discussion Highlights"
                    },
                    {
                      "type": "span",
                      "value": ":"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Language Translation"
                            },
                            {
                              "type": "span",
                              "value": ": Alex Volkov highlights the surprising lack of focus on multilingual capabilities in the recent models, especially given Meta's earlier successes in this domain."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Llama2's Coding Abilities"
                            },
                            {
                              "type": "span",
                              "value": ": Swyx mentions the inadequacies of relying solely on human eval for coding chatbot benchmarks, stressing the need for newer benchmarks."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Training Data Transparency"
                            },
                            {
                              "type": "span",
                              "value": ": Simon Willison addresses the challenges faced by consumers due to the opaqueness of the models, emphasizing the importance of understanding their training data."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Meta's Release Strategy"
                            },
                            {
                              "type": "span",
                              "value": ": Alex Volkov and Nathan Lambert delve into the implications of Meta's open-source release, considering both its potential commercial benefits and the challenges associated with regulatory bodies."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Role of Open Licensing"
                            },
                            {
                              "type": "span",
                              "value": ": Simon Willison mentions the pivotal role Meta AI has in openly licensed language model research, particularly with the introduction of LAMA."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Impact on Startups"
                            },
                            {
                              "type": "span",
                              "value": ": Matt Bornstein discusses the monumental effect on the startup ecosystem, pointing out the ongoing dilemma faced by startups regarding the use of off-the-shelf models or custom training."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Throughout the discussion, the experts contemplate the balance between commercial benefits, the importance of transparency, and the evolving needs of the AI community, given the rapid advancements in the field."
                    }
                  ]
                }
              ]
            }
          }
        },
        "topImages": [
          {
            "basename": "abls4-img3",
            "height": 936,
            "width": 936,
            "filename": "abls4-img3.png",
            "format": "png",
            "alt": null,
            "url": "https://www.datocms-assets.com/101962/1691982805-abls4-img3.png"
          }
        ]
      },
      {
        "mainContent": {
          "value": {
            "schema": "dast",
            "document": {
              "type": "root",
              "children": [
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "Discussing the Evolution and Transparency of AI Models, and the Importance of Preference Data in Training"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "marks": [
                        "strong"
                      ],
                      "value": "Trying Llama 2 Model:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "value": "Hugging Face has launched an inference endpoint for the Llama 2 model, one of the few ways to access the 70B model directly. The G GML ecosystem is a reference for the two-bit quantized version. Base 10 might also have a similar setup."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "marks": [
                        "strong"
                      ],
                      "value": "Dataset Transparency Debate:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "value": "While some believe that we don't necessarily need to know full datasets of models like Llama if we can evaluate them properly, others argue for transparency. Simon Willison favors transparency, especially when comparing two similar models. The emerging theme seems to be dataset non-transparency, highlighted by challenges such as Falcon's controversial responses to sensitive topics."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "marks": [
                        "strong"
                      ],
                      "value": "Cost of Developing Llama 2:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "value": "The cost associated with Llama 2, estimated around $25 million, primarily revolves around preference data collection rather than GPU resources. Nathan Lambert discusses the nuances of data collection costs, including the complexities associated with the iterative, hands-on approach used for data acquisition."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "marks": [
                        "strong"
                      ],
                      "value": "Human Annotators and Model Annotation:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "value": "There is a trend towards shifting from pre-training datasets to preference and HF data. An interesting observation is that human annotators often end up using models for annotation, leading to a recursive scenario of \"models rating models.\" The need for diversity in annotation tasks, ensuring there isn't much overlap, is emphasized. Models are getting good enough that some, like those from Philanthropic, no longer require supervised fine-tuning."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "marks": [
                        "strong"
                      ],
                      "value": "Preference Models and Open Source:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "value": "There is a gap in the open-source community when it comes to preference models. Nathan Lambert suggests that the focus should be on the preference side, potentially enabling creative avenues like constitutional AI. The possibility of creating a Stack Overflow-like platform for Llama to gather code dataset through ratings is also touched upon."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "The conversation explores the intricacies of AI models, the importance of data transparency, and the cost factors associated with model development. Preference data collection emerges as a significant theme, highlighting the need for diverse and high-quality datasets."
                    }
                  ]
                }
              ]
            }
          }
        },
        "topImages": [
          {
            "basename": "abls4-img5",
            "height": 936,
            "width": 936,
            "filename": "abls4-img5.png",
            "format": "png",
            "alt": null,
            "url": "https://www.datocms-assets.com/101962/1691983359-abls4-img5.png"
          }
        ]
      },
      {
        "mainContent": {
          "value": {
            "schema": "dast",
            "document": {
              "type": "root",
              "children": [
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "Llama 2 Finetuning Ecosystem Discussion"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "marks": [
                        "strong"
                      ],
                      "value": "Alex Volkov"
                    },
                    {
                      "type": "span",
                      "value": " emphasizes that the Llama 2 ecosystem offers improved ease of work due to existing infrastructure like G GML and Pinocchio browsers. Commercial users who are profit-driven will also find it easier to participate now. Companies like scale.AI have already started providing open source toolkits for fine-tuning Llama on platforms such as Databricks."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "marks": [
                        "strong"
                      ],
                      "value": "Nathan Lambert"
                    },
                    {
                      "type": "span",
                      "value": " notes the dedication of teams like Hugging Face, who worked extensively to provide day zero support for Llama 2."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "marks": [
                        "strong"
                      ],
                      "value": "Swyx"
                    },
                    {
                      "type": "span",
                      "value": " brings up Scale.AI's partnership announcement, suggesting a connection with Llama 2, though it wasn't explicitly stated."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "marks": [
                        "strong"
                      ],
                      "value": "Simon Willison"
                    },
                    {
                      "type": "span",
                      "value": " expresses a desire for straightforward guidance on running Llama 2 on Mac's M2 using the Hugging Face Transformers for better GPU utilization. Nathan mentions Pedro from Hugging Face is already working towards integrating with Apple's ecosystem."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "marks": [
                        "strong"
                      ],
                      "value": "Russell Kaplan"
                    },
                    {
                      "type": "span",
                      "value": " from Scale.AI mentions a new open-source library, LM engine, to aid in the fine-tuning of Llama 2 and other language models. Scale.AI aims to fine-tune Llama 2 for domain-specific tasks, such as SQL and retrieval."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "marks": [
                        "strong"
                      ],
                      "value": "Simon Willison"
                    },
                    {
                      "type": "span",
                      "value": " requests a Llama 2 based version of the ChatGPT code interpreter."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Throughout the discussion, there's a recurring theme of the community's excitement about the possibilities with Llama 2 and the broader implications for the AI industry."
                    }
                  ]
                }
              ]
            }
          }
        },
        "topImages": [
          {
            "basename": "abls4-img4",
            "height": 936,
            "width": 936,
            "filename": "abls4-img4.png",
            "format": "png",
            "alt": null,
            "url": "https://www.datocms-assets.com/101962/1691983398-abls4-img4.png"
          }
        ]
      },
      {
        "mainContent": {
          "value": {
            "schema": "dast",
            "document": {
              "type": "root",
              "children": [
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "Partnerships, Predictions & Conclusion"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "The discussion revolves around relationships and partnerships in the tech world, particularly focusing on OpenAI, Azure, and Meta's contributions to open source AI models. Here are the main points:"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "marks": [
                        "strong"
                      ],
                      "value": "Relationship Dynamics"
                    },
                    {
                      "type": "span",
                      "value": ":"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "value": "Anton observes a potential shift in friendship dynamics among tech leaders like Sam Altman, Mark Zuckerberg, and Satya Nadella, pointing out a notable interaction between Satya and Mark."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "value": "Alessio confirms this, recalling a recent photo of Satya laughing with Mark, contrasting it with a more serious picture of Satya with Sam."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "marks": [
                        "strong"
                      ],
                      "value": "Azure's Role with OpenAI"
                    },
                    {
                      "type": "span",
                      "value": ":"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "value": "OpenAI's heavy reliance on Azure as a hardware platform is highlighted."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "value": "Anton and swyx ponder the significance of Azure being the launch partner for OpenAI, especially given Microsoft's considerable investment in OpenAI via Azure credits."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "value": "The importance of privacy and control is emphasized, with Russell discussing how businesses might prefer running inferences on their own Azure hardware to maintain data privacy and security."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "marks": [
                        "strong"
                      ],
                      "value": "Open Source Movement"
                    },
                    {
                      "type": "span",
                      "value": ":"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "value": "The topic shifts to open sourcing in AI, with Matt pointing out that being the best in open source can sometimes outweigh being the top in proprietary models."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "value": "Swyx highlights the excitement around open source models and seeks predictions for the future of this domain."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "marks": [
                        "strong"
                      ],
                      "value": "Predictions"
                    },
                    {
                      "type": "span",
                      "value": ":"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "value": "Anton believes that the AI community will explore the true capabilities of the models, possibly uncovering new uses and furthering research in embeddings and internal states."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "value": "Nathan highlights the significance of this open source move for research, allowing it to proceed unhindered."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "value": "Simon eagerly anticipates the creative fine-tuning of the model and the potential for breakthrough applications."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "value": "Russell predicts a surge in domain-specific fine-tuning, leading to specialized AI agents proficient in particular tools or tasks."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "value": "Anton expresses optimism about the future of AI agents, hinting at their practical use cases."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "The discussion closes with participants sharing their bullish sentiments on the advancements of open source models, especially with regard to AI agents and their potential applications."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "\n"
                    },
                    {
                      "type": "span",
                      "marks": [
                        "strong"
                      ],
                      "value": "Ai Doom:"
                    },
                    {
                      "type": "span",
                      "value": "\nIn a discussion concerning the potential impact and implications of releasing new AI material, specifically the LAMA2 model, participants shared diverse views. While some displayed apathy toward potential doom scenarios, others expressed optimism, suggesting that accessibility to the model's internals might enhance safety and understanding. There was an acknowledgment that language models, which were once considered cutting edge, are quickly becoming more commonplace, with a future where many devices could run them natively. The conversation concluded with swyx encouraging everyone to experiment with LAMA2, and gratitude was expressed all around before signing off."
                    }
                  ]
                }
              ]
            }
          }
        },
        "topImages": [
          {
            "basename": "abls4-img6",
            "height": 936,
            "width": 936,
            "filename": "abls4-img6.png",
            "format": "png",
            "alt": null,
            "url": "https://www.datocms-assets.com/101962/1691983112-abls4-img6.png"
          }
        ]
      }
    ],
    "seo": {
      "description": "Dive into datasets, benchmarks & AI predictions. Llama insights & drama await in this top podcast!",
      "title": "Latent Space Podcast 7/19/23 [Summary] - Llama 2",
      "twitterCard": null,
      "image": {
        "width": 1632,
        "height": 574,
        "title": null,
        "alt": null,
        "url": "https://www.datocms-assets.com/101962/1691968295-screenshot-2023-08-13-at-7-11-06-pm.png"
      }
    }
  },
  "topics": [
    "Summary",
    "LLM",
    "Open Source",
    "Small Models"
  ],
  "shortLink": ""
}