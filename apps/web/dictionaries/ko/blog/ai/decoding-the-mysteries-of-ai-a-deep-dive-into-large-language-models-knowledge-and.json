{
  "relatedBlogs": [
    {
      "id": "190259319",
      "topics": [
        "Summary",
        "LLM",
        "Training"
      ],
      "title": "Latent Space Podcast 8/16/23 [Summary] - The Mathematics of Training LLMs — with Quentin Anthony of Eleuther AI",
      "slug": "latent-space-podcast-8-16-23-summary-the-mathematics-of-training-llms-with-que",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T09:19:45+01:00",
      "description": "Explore the math behind training LLMs with Quentin Anthony from Eleuther AI. Dive into the Transformers Math 101 article & master distributed training techniques for peak GPU performance.",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692324088-screenshot-2023-08-17-at-9-59-17-pm.png"
      },
      "seo": {
        "description": "Dive into the Transformers Math 101 article & master distributed training techniques for peak GPU performance.",
        "title": "Latent Space Podcast 8/16/23 [Summary] Math of Training LLMs",
        "twitterCard": null,
        "image": {
          "width": 1576,
          "height": 554,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1692324088-screenshot-2023-08-17-at-9-59-17-pm.png"
        }
      }
    },
    {
      "id": "190259129",
      "topics": [
        "LLM",
        "Hardware",
        "Summary",
        "Edge"
      ],
      "title": "Latent Space Podcast 8/10/23 [Summary]: LLMs Everywhere: Running 70B models in browsers and iPhones using MLC — with Tianqi Chen of CMU / OctoML",
      "slug": "latent-space-podcast-8-10-23-summary-llms-everywhere-running-70b-models-in-browse",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T09:18:37+01:00",
      "description": "Explore the magic of MLC with Tianqi Chen: deploying 70B models on browsers & iPhones. Dive into XGBoost, TVM's creation, & the future of universal AI deployments. ",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1691894611-screenshot-2023-08-12-at-10-42-43-pm.png"
      },
      "seo": {
        "description": "Explore deploying 70B models on browsers & iPhones. Dive into XGBoost, TVM's creation, & the future of universal AI deployments. ",
        "title": "Latent Space 8/10/23 [Summary]: LLMs Everywhere",
        "twitterCard": null,
        "image": {
          "width": 1538,
          "height": 548,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1691894611-screenshot-2023-08-12-at-10-42-43-pm.png"
        }
      }
    },
    {
      "id": "190259087",
      "topics": [
        "Summary",
        "LLM",
        "Code",
        "Open Source",
        "Small Models"
      ],
      "title": "Latent Space Podcast 8/4/23 [Summary] Latent Space x AI Breakdown crossover pod! ",
      "slug": "latent-space-podcast-8-4-23-summary-latent-space-x-ai-breakdown-crossover-pod",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T09:16:33+01:00",
      "description": "Join AI Breakdown & Latent Space for the summer AI tech roundup: Dive into GPT4.5, Llama 2, AI tools, the rising AI engineer, and more!",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1691539617-screenshot-2023-08-08-at-8-02-52-pm.png"
      },
      "seo": {
        "description": "Dive into GPT4.5, Llama 2, AI tools, the rising AI engineer, and more!",
        "title": "Latent Space Podcast 8/4/23 [Summary] AI Breakdown crossover",
        "twitterCard": null,
        "image": {
          "width": 1578,
          "height": 558,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1691539617-screenshot-2023-08-08-at-8-02-52-pm.png"
        }
      }
    },
    {
      "id": "190259172",
      "topics": [
        "Summary",
        "LLM",
        "Open Source",
        "Small Models"
      ],
      "title": "Latent Space Podcast 7/19/23 [Summary] - Llama 2: The New Open LLM SOTA (ft. Nathan Lambert, Matt Bornstein, Anton Troynikov, Russell Kaplan, Whole Mars Catalog et al.)",
      "slug": "latent-space-podcast-7-19-23-summary-llama-2-the-new-open-llm-sota-ft-nathan-lamb",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T09:12:38+01:00",
      "description": "Explore Llama 2, the latest AI breakthrough with experts Nathan Lambert, Matt Bornstein & more. Dive into datasets, benchmarks & AI predictions. Llama insights & drama await in this top podcast!",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1691968295-screenshot-2023-08-13-at-7-11-06-pm.png"
      },
      "seo": {
        "description": "Dive into datasets, benchmarks & AI predictions. Llama insights & drama await in this top podcast!",
        "title": "Latent Space Podcast 7/19/23 [Summary] - Llama 2",
        "twitterCard": null,
        "image": {
          "width": 1632,
          "height": 574,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1691968295-screenshot-2023-08-13-at-7-11-06-pm.png"
        }
      }
    },
    {
      "id": "190259191",
      "topics": [
        "Summary",
        "Code",
        "LLM"
      ],
      "title": "Latent Space Podcast 7/10/23 [Summary] - Code Interpreter == GPT 4.5 (w/ Simon Willison, Alex Volkov, Aravind Srinivas, Alex Graveley, et al.)",
      "slug": "latent-space-podcast-7-10-23-summary-code-interpreter-gpt-4-5-w-simon-willison-al",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T09:09:26+01:00",
      "description": "Explore ChatGPT's Code Interpreter: a game-changer in AI. Dive into its 1000x capabilities leap with Simon, Alex & top AI experts. #CodeAugmentedInference #GPT4_5",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692048911-screenshot-2023-08-14-at-3-34-05-pm.png"
      },
      "seo": {
        "description": "Explore ChatGPT's Code Interpreter: a game-changer in AI. Dive into its 1000x capabilities leap with Simon, Alex & top AI experts. ",
        "title": "Latent Space Podcast  [Summary] Code Interpreter = GPT 4.5",
        "twitterCard": null,
        "image": {
          "width": 1596,
          "height": 582,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1692048911-screenshot-2023-08-14-at-3-34-05-pm.png"
        }
      }
    },
    {
      "id": "190259238",
      "topics": [
        "Hardware",
        "LLM",
        "Summary"
      ],
      "title": "Latent Space Podcast 6/20/23 [Summary] - Commoditizing the Petaflop — with George Hotz of the tiny corp",
      "slug": "latent-space-podcast-6-20-23-summary-commoditizing-the-petaflop-with-george-ho",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T09:07:15+01:00",
      "description": "George Hotz of tiny corp challenges Nvidia & Google! Dive into the world of AMD collaborations, insights on ggml, Mojo, Elon & GPT-4, plus a peek into AI Girlfriend. ",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692154615-screenshot-2023-08-15-at-10-55-40-pm.png"
      },
      "seo": {
        "description": "George Hotz of tiny corp challenges Nvidia & Google! AMD collaborations, insights on ggml, Mojo, Elon & GPT-4, plus a peek into AI Girlfriend. ",
        "title": "Latent Space Podcast 6/20/23 [Summary] - George Hotz ",
        "twitterCard": null,
        "image": {
          "width": 1586,
          "height": 508,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1692154615-screenshot-2023-08-15-at-10-55-40-pm.png"
        }
      }
    },
    {
      "id": "190259294",
      "topics": [
        "LLM",
        "Functions",
        "Summary"
      ],
      "title": "Latent Space Podcast 6/14/23 [Summary] - Emergency Pod: OpenAI's new Functions API, 75% Price Drop, 4x Context Length (w/ Alex Volkov, Simon Willison, Riley Goodside, Joshua Lochner, Stefania Druga, Eric Elliott, Mayo Oshin et al)",
      "slug": "latent-space-podcast-6-14-23-summary-emergency-pod-openai-s-new-functions-api-75",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T09:05:04+01:00",
      "description": "Explore the June 2023 OpenAI updates with top AI engineers from Scale, Microsoft, Pinecone, & Huggingface. Dive into the Code x LLM paradigms and discover Recursive Function Agents.",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692221668-screenshot-2023-08-16-at-5-32-29-pm.png"
      },
      "seo": {
        "description": "Explore the June 2023 OpenAI updates with top AI engineers from Scale, Microsoft, Pinecone, & Huggingface. ",
        "title": "Latent Space Podcast 6/20/23 [Summary] - Emergency Pod",
        "twitterCard": null,
        "image": {
          "width": 1626,
          "height": 606,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1692221668-screenshot-2023-08-16-at-5-32-29-pm.png"
        }
      }
    },
    {
      "id": "190259333",
      "topics": [
        "LLM",
        "Summary",
        "UX"
      ],
      "title": "Latent Space Podcast 6/8/23 [Summary] - From RLHF to RLHB: The Case for Learning from Human Behavior - with Jeffrey Wang and Joe Reeve of Amplitude",
      "slug": "latent-space-podcast-6-8-23-summary-from-rlhf-to-rlhb-the-case-for-learning-from",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T09:02:33+01:00",
      "description": "Explore AI & analytics with Jeffrey Wang & Joe Reeve on Latent Space Live! Dive into why AI values Analytics and the power of first-party behavioral data. ",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692386432-screenshot-2023-08-18-at-3-17-04-pm.png"
      },
      "seo": {
        "description": "Explore AI & analytics with Jeffrey Wang & Joe Reeve on Latent Space Live! Dive into why AI values Analytics and the power of first-party behavioral data. ",
        "title": "Latent Space Podcast 6/8/23 [Summary] - From RLHF to RLHB",
        "twitterCard": null,
        "image": {
          "width": 1674,
          "height": 550,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1692386432-screenshot-2023-08-18-at-3-17-04-pm.png"
        }
      }
    },
    {
      "id": "190260528",
      "topics": [
        "Summary",
        "LLM",
        "UX"
      ],
      "title": "Latent Space Podcast 6/1/23 [Summary] - Building the AI × UX Scenius — with Linus Lee of Notion AI",
      "slug": "latent-space-podcast-6-1-23-summary-building-the-ai-x-ux-scenius-with-linus-le",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T09:00:12+01:00",
      "description": "Explore Notion AI's transformative approach to AI and UX. Dive into the future of AI-augmented workspaces, the value beyond chat interfaces, and insights on effective knowledge work. Recap of AI×UX NYC meetup included!",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692390655-screenshot-2023-08-18-at-4-28-51-pm.png"
      },
      "seo": {
        "description": "Explore Notion AI's transformative approach to AI and UX. ",
        "title": "Latent Space Podcast 6/1/23 [Summary] - AI × UX Scenius",
        "twitterCard": null,
        "image": {
          "width": 1614,
          "height": 546,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1692390655-screenshot-2023-08-18-at-4-28-51-pm.png"
        }
      }
    },
    {
      "id": "190260557",
      "topics": [
        "Summary",
        "Code",
        "LLM",
        "Agents"
      ],
      "title": "Latent Space Podcast 5/25/23 [Summary] - Debugging the Internet with AI agents – with Itamar Friedman of Codium AI and AutoGPT",
      "slug": "latent-space-podcast-5-25-23-summary-debugging-the-internet-with-ai-agents-with",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T08:58:27+01:00",
      "description": "Explore the future of AI with Itamar Friedman from Codium AI on 'Debugging the Internet'. Dive into 'Extreme DRY' agents, the rapid sync of specs & tests, and the balance between code & testing. Plus, insights from Toran & an exclusive look at AutoGPT's roadmap!",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692397413-screenshot-2023-08-18-at-6-10-09-pm.png"
      },
      "seo": {
        "description": "Dive into 'Extreme DRY' agents, the rapid sync of specs & tests, and the balance between code & testing. ",
        "title": "Latent Space Pod 5/25/23 [Summary] Debugging the Internet",
        "twitterCard": null,
        "image": {
          "width": 1568,
          "height": 548,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1692397413-screenshot-2023-08-18-at-6-10-09-pm.png"
        }
      }
    },
    {
      "id": "190260577",
      "topics": [
        "LLM",
        "Small Models"
      ],
      "title": "Latent Space Podcast 5/20/23 [Summary] - MPT-7B and The Beginning of Context=Infinity — with Jonathan Frankle and Abhinav Venigalla of MosaicML",
      "slug": "latent-space-podcast-5-20-23-summary-mpt-7b-and-the-beginning-of-context-infinity",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T08:57:33+01:00",
      "description": "Dive into MosaicML's 9-day, $200k \"llongboi\" MPT-7B training, data prep insights, & the rise of open AI models with experts Frankle & Venigalla.",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692409795-screenshot-2023-08-18-at-9-49-21-pm.png"
      },
      "seo": {
        "description": "Dive into MosaicML's 9-day, $200k \"llongboi\" MPT-7B training, data prep insights, & the rise of open AI models ",
        "title": "Latent Space Podcast 6/25/23 [Summary] MosaicML",
        "twitterCard": null,
        "image": {
          "width": 1568,
          "height": 556,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1692409795-screenshot-2023-08-18-at-9-49-21-pm.png"
        }
      }
    },
    {
      "id": "190260597",
      "topics": [
        "LLM",
        "Structured Data"
      ],
      "title": "Latent Space Podcast 5/15/23 [Summary] - Guaranteed quality and structure in LLM outputs - with Shreya Rajpal of Guardrails AI",
      "slug": "latent-space-podcast-5-15-23-summary-guaranteed-quality-and-structure-in-llm-outp",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T08:56:26+01:00",
      "description": "Explore Ep. 12 with Shreya Rajpal of Guardrails AI: Dive deep into validating LLM outputs, refining answers through re-asking loops, and establishing SLAs for models. Master the nuances of AI quality assurance.",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692495732-screenshot-2023-08-19-at-9-38-27-pm.png"
      },
      "seo": {
        "description": "Explore Ep. 12 with Shreya Rajpal of Guardrails AI: Dive deep into validating LLM outputs.",
        "title": "Latent Space Podcast 5/15/23 [Summary] Quality LLM Outputs",
        "twitterCard": null,
        "image": {
          "width": 1580,
          "height": 512,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1692495732-screenshot-2023-08-19-at-9-38-27-pm.png"
        }
      }
    },
    {
      "id": "190260606",
      "topics": [
        "LLM",
        "Training",
        "Agents",
        "Multimodal"
      ],
      "title": "Latent Space Podcast 5/8/23 [Summary] - The AI Founder Gene: Being Early, Building Fast, and Believing in Greatness — with Sharif Shameem of Lexica",
      "slug": "latent-space-podcast-5-8-23-summary-the-ai-founder-gene-being-early-building-fast",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T08:52:09+01:00",
      "description": "Ep.11 with Sharif Shameem of Lexica: Dive into the AI founder mindset, uncovering the secrets to pioneering innovation, building game-changing tech, training models, and the intriguing potential of Agents and genomic sequencing. ",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692501984-screenshot-2023-08-19-at-11-24-05-pm.png"
      },
      "seo": {
        "description": "Ep.11 with Sharif Shameem of Lexica: Dive into the AI founder mindset, uncovering the secrets to pioneering innovation.",
        "title": "Latent Space Pod 5/8/23 [Summary] The AI Founder Gene",
        "twitterCard": null,
        "image": {
          "width": 1606,
          "height": 550,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1692501984-screenshot-2023-08-19-at-11-24-05-pm.png"
        }
      }
    },
    {
      "id": "190260640",
      "topics": [
        "Summary",
        "Open Source",
        "LLM"
      ],
      "title": "Latent Space Podcast 5/5/23 [Summary] - No Moat: Closed AI gets its Open Source wakeup call — ft. Simon Willison",
      "slug": "latent-space-podcast-5-5-23-summary-no-moat-closed-ai-gets-its-open-source-wakeup",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T08:49:18+01:00",
      "description": "Explore 'No Moat: Closed AI's Open Source Awakening' with Simon Willison. Dive into leaked Google Moat memo insights, Google Brain Drain, and Python's speed boost with Mojo.",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692566921-screenshot-2023-08-20-at-5-25-53-pm.png"
      },
      "seo": {
        "description": "Explore 'No Moat: Closed AI's Open Source Awakening' with Simon Willison. Dive into leaked Google Moat memo insights.",
        "title": "Latent Space Podcast 5/5/23 [Summary] - No Moat",
        "twitterCard": null,
        "image": {
          "width": 1602,
          "height": 532,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1692566921-screenshot-2023-08-20-at-5-25-53-pm.png"
        }
      }
    },
    {
      "id": "190260671",
      "topics": [
        "LLM",
        "Code",
        "Summary"
      ],
      "title": "Latent Space Podcast 5/3/23 [Summary] - Training a SOTA Code LLM in 1 week and Quantifying the Vibes — with Reza Shabani of Replit",
      "slug": "latent-space-podcast-5-3-23-summary-training-a-sota-code-llm-in-1-week-and-quanti",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T08:46:08+01:00",
      "description": "Ep. 10 with Reza Shabani: Dive deep into the rapid training of a state-of-the-art Code LLM, explore Replit Ghostwriter's future, and journey from Finance to AI. Discover the transition from Kaplan to Chinchilla and more!",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692584998-screenshot-2023-08-20-at-10-17-26-pm.png"
      },
      "seo": {
        "description": "Ep. 10 with Reza Shabani: Dive deep into the rapid training of a state-of-the-art Code LLM!",
        "title": "Latent Space Pod 5/3/23 [Summary] - SOTA Code LLM",
        "twitterCard": null,
        "image": {
          "width": 1566,
          "height": 530,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1692584998-screenshot-2023-08-20-at-10-17-26-pm.png"
        }
      }
    },
    {
      "id": "190629271",
      "topics": [
        "LLM",
        "Small Models",
        "Summary"
      ],
      "title": "Latent Space Podcast 4/28/23 [Summary] - Mapping the future of *truly* Open Models and Training Dolly for $30 — with Mike Conover of Databricks",
      "slug": "latent-space-podcast-4-28-23-summary-mapping-the-future-of-truly-open-models-and",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T08:33:12+01:00",
      "description": "Explore the future of open models with Mike Conover of Databricks. Dive deep into Dolly's creation, its transition from 1.0 to 2.0, & the influences behind its development. Ep.9 touches on model infrastructure, Databricks' vision, & more. #AI #OpenModels #Dolly",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1694038707-screenshot-2023-09-06-at-3-12-24-pm.png"
      },
      "seo": {
        "description": "Ep.9 touches on model infrastructure, Databricks' vision, & more. #AI #OpenModels #Dolly",
        "title": "Latent Space Pod 4/28/23 [Summary] - Mike of Databricks",
        "twitterCard": null,
        "image": {
          "width": 1572,
          "height": 628,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1694038707-screenshot-2023-09-06-at-3-12-24-pm.png"
        }
      }
    },
    {
      "id": "191164291",
      "topics": [
        "LLM",
        "Enterprise",
        "Summary"
      ],
      "title": "Latent Space Podcast 4/21/23 [Summary] - AI-powered Search for the Enterprise — with Deedy Das of Glean",
      "slug": "latent-space-podcast-4-21-23-summary-ai-powered-search-for-the-enterprise-with",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T08:31:31+01:00",
      "description": "Ep.8: Dive into AI in enterprise search with Deedy Das of Glean. Unpack challenges in creating an AI search giant, Google vs ChatGPT comparisons, AI infrastructure intricacies, spotting AI-generated text, and why businesses need more than just Document QA.",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1694134074-screenshot-2023-09-07-at-5-43-48-pm.png"
      },
      "seo": {
        "description": "Ep.8: Dive into AI in enterprise search with Deedy Das of Glean. Unpack challenges in creating an AI search giant, Google vs ChatGPT ...",
        "title": "Latent Space Podcast 4/21/23 [Summary] - with Deedy Das ",
        "twitterCard": null,
        "image": {
          "width": 1608,
          "height": 530,
          "title": null,
          "alt": null,
          "url": "https://www.datocms-assets.com/101962/1694134074-screenshot-2023-09-07-at-5-43-48-pm.png"
        }
      }
    }
  ],
  "blogContent": {
    "id": "190258672",
    "topics": [
      "Getting Started",
      "LLM"
    ],
    "title": "Decoding the Mysteries of AI: A Deep Dive into Large Language Models' Knowledge and Limitations",
    "slug": "decoding-the-mysteries-of-ai-a-deep-dive-into-large-language-models-knowledge-and",
    "authorName": "Prof. Otto Nomos",
    "authorPicture": {
      "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
    },
    "_publishedAt": "2023-10-05T05:51:17+01:00",
    "description": "Unravel the enigma of Large Language Models (LLMs). Understand how they acquire knowledge, their cutoffs, limits, and the potential pitfalls on your AI journey.\"",
    "thumbnail": {
      "url": "https://www.datocms-assets.com/101962/1690333798-ab4-img1.png"
    },
    "contentBlock": [
      {
        "mainContent": {
          "value": {
            "schema": "dast",
            "document": {
              "type": "root",
              "children": [
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "Introduction"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Welcome, future pioneers of artificial intelligence! Do you feel the crackling energy of an explorer, standing on the precipice of the known world, ready to venture into the unknown? Well, grab your metaphorical compass and join me as we dive deep into the uncharted waters of LLMs!"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "You see, LLMs, the powerful vessels of AI, hold immense potential. But with every treasure chest, there's a lock - and understanding the nuances of these models, how they learn, their limitations, and how to navigate them, is your golden key. It's an adventure waiting to happen!"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Our expedition will carry us through the labyrinthine landscapes of knowledge acquisition in LLMs, the mysteries of the knowledge cutoff, and the shadowed valleys of their limitations. We’ll uncover how these advanced models learn, where their education halts, and what happens beyond that point."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "So, tighten your grip, ensure your safety ropes are secure, and prepare for the exhilarating thrill of exploration. We're about to embark on a journey deep into the heart of AI's greatest enigmas - LLM's knowledge and its limitations. By the end, you'll be well-equipped to navigate this complex maze, ready to turn every corner, climb every wall, and hurdle every limitation with confidence and prowess. Are you ready? Then let's embark on this thrilling journey together!"
                    }
                  ]
                }
              ]
            }
          }
        },
        "topImages": [
          {
            "basename": "ab4-img1",
            "height": 870,
            "width": 870,
            "filename": "ab4-img1.png",
            "format": "png",
            "alt": null,
            "url": "https://www.datocms-assets.com/101962/1690333798-ab4-img1.png"
          }
        ]
      },
      {
        "mainContent": {
          "value": {
            "schema": "dast",
            "document": {
              "type": "root",
              "children": [
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "How LLMs Acquire Knowledge"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Embarking on this journey of understanding, let's cast our minds to the roots of a mighty tree. Just as these roots draw nutrients from the soil, LLMs draw their knowledge from a vast sea of data. Every blog post, every article, every snippet of text absorbed by the LLM is a droplet of knowledge, transforming it into a fount of information."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "marks": [
                        "strong"
                      ],
                      "value": "So, how does this data become knowledge?"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "It all lies in the intricate ballet of training. Imagine a maestro teaching an orchestra – an LLM is akin to the orchestra, the conductor is the machine learning algorithm, and the symphony they strive to perfect is the ability to generate human-like text. The conductor guides and corrects, training the orchestra to produce a harmonious melody."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Similarly, during the training phase, LLMs ingest a colossal corpus of text data, learning from patterns and structures within it. This is not a process of cramming facts but a journey of internalizing the rhythm and melody of human language. It's a poetic performance of the algorithm and data dancing in perfect synchrony, giving birth to knowledge within the LLM."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "marks": [
                        "strong"
                      ],
                      "value": "However, there is a critical distinction to grasp."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "The difference between programmed knowledge and learned knowledge is akin to a parrot mimicking speech and a child learning to talk. A parrot can reproduce sounds, but it does not comprehend their meaning. A child, on the other hand, not only learns words but grasps their implications, uses them in context, and evolves their vocabulary over time."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "The same applies to LLMs. Their 'knowledge' is not programmed in the sense that they know specific facts about the world. Instead, they recognize patterns and generate responses based on those patterns. They 'learn' to generate text that statistically resembles the data they were trained on."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Like a well-trained artist, an LLM can paint a masterpiece that mimics the grandeur of Van Gogh or the subtlety of Vermeer, despite never having seen a sunrise or the play of light on a pearl earring. This resemblance is learned knowledge - an imitation of reality, not a comprehension of it."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Grasping these concepts is your first step in navigating the labyrinth of LLMs. As explorers of AI, it’s crucial that we don’t just marvel at the heights these models can reach but also delve deep into their roots - the essence of their learning. This understanding is your map to the AI world, the key to harnessing the power of LLMs."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "We’ve just embarked on this journey of understanding, so buckle up, keep your explorer’s hat on, and prepare to delve deeper. Next, we shall unravel the enigmatic concept of a 'knowledge cutoff' and how it shapes the limits of an LLM's world."
                    }
                  ]
                }
              ]
            }
          }
        },
        "topImages": [
          {
            "basename": "ab4-img2",
            "height": 870,
            "width": 870,
            "filename": "ab4-img2.png",
            "format": "png",
            "alt": null,
            "url": "https://www.datocms-assets.com/101962/1690333892-ab4-img2.png"
          }
        ]
      },
      {
        "mainContent": {
          "value": {
            "schema": "dast",
            "document": {
              "type": "root",
              "children": [
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "Demystifying the Knowledge Cutoff"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Stepping into the labyrinthine library of Large Language Models, one quickly encounters the curious concept of the 'knowledge cutoff.' This elusive concept, as mystifying as it sounds, is rather straightforward. The knowledge cutoff of an LLM is the date at which the model's training data ends. It's the year the librarians stopped adding books to our vast library."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "marks": [
                        "strong"
                      ],
                      "value": "Why is this cutoff significant?"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Consider a historian who stopped studying history after 1800. Their knowledge about the world post-1800 would be based on conjecture, not actual information. Similarly, an LLM trained up to 2021 would not know about events, discoveries, or cultural shifts that occurred in 2023."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "The knowledge cutoff significantly impacts an LLM's performance. After this date, the model can't generate accurate or relevant information because it simply doesn't know it. It's like asking our historian about the War of 1812 – they can make educated guesses based on the context they know, but their knowledge is fundamentally incomplete."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "marks": [
                        "strong"
                      ],
                      "value": "Navigating the knowledge cutoff."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Working with an LLM's knowledge cutoff requires understanding and finesse. It's crucial to remember that, like our historian, the LLM cannot update its knowledge post-cutoff. So, asking it about events or facts that emerged after the cutoff is like asking a sailor about the desert. The response may sound plausible, but it's ultimately a work of fiction."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "However, this does not discount the value that LLMs provide. Think of them as brilliant oracles of the past. They are deeply knowledgeable about the world up to a certain point in time. They can provide insightful commentary, answer complex questions, generate creative text, and much more, all within their knowledge domain."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Working effectively with LLMs, then, is a dance of directing questions appropriately, steering the conversation towards areas the model understands, and cross-checking information for relevance and accuracy."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Remember, LLMs are tools in your toolbox, not infallible sources of wisdom. Learning to use these tools effectively is not just about mastering the technology; it's about understanding their knowledge boundaries and creatively navigating within them."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "The journey continues as we now delve into the labyrinth's deeper realms. As we tread further, we will uncover the limits and potential pitfalls of LLMs, helping you become not just a user of AI, but a true master. Brace yourselves, for the expedition delves deeper!"
                    }
                  ]
                }
              ]
            }
          }
        },
        "topImages": [
          {
            "basename": "ab4-img3",
            "height": 870,
            "width": 870,
            "filename": "ab4-img3.png",
            "format": "png",
            "alt": null,
            "url": "https://www.datocms-assets.com/101962/1690333961-ab4-img3.png"
          }
        ]
      },
      {
        "mainContent": {
          "value": {
            "schema": "dast",
            "document": {
              "type": "root",
              "children": [
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "Unveiling the Limits of LLMs"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Even as Large Language Models (LLMs) marvel us with their abilities, we must remember that they, like our master painter, have their limitations. They are not omniscient beings but tools with boundaries defined by their training."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "marks": [
                        "strong"
                      ],
                      "value": "Where do LLMs fall short?"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "The first limit is their understanding of context. An LLM can read and generate text, but it doesn't truly \"understand\" in the way humans do. It cannot empathize with emotions or decipher the subtleties of humor and sarcasm."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Secondly, LLMs can't generate new, verifiable knowledge. They can't predict the stock market, give real-time weather updates, or create innovative scientific theories. LLMs are not oracles of the future or creators of unlearned knowledge; they merely reflect the information they've been trained on."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Lastly, LLMs can inadvertently generate biased or harmful content. Even though they don't have beliefs or intentions, they can reflect the biases inherent in their training data, much like a mirror reflecting the world without understanding it."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "marks": [
                        "strong"
                      ],
                      "value": "What risks lurk in these limitations, and how can we navigate them?"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Misunderstanding the limits of LLMs can lead to inaccurate conclusions or decisions based on the information they generate. Trusting an LLM's prediction about tomorrow's weather or the future of Bitcoin is like trusting our master painter's sketch of an unseen future - it's mere conjecture."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Biased output from LLMs can also perpetuate harmful stereotypes or misinformation, making critical analysis of their output crucial."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Mitigation begins with education. Understand these limitations. Use LLMs for what they are excellent at – generating text based on a massive array of pre-existing data. Cross-check the information they provide, especially when it pertains to sensitive or critical matters."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Remember, in the world of AI, you are the master, and the LLM is your tool. Use it with understanding, wield it with care, and together, you can create something truly remarkable. It's this awareness, this dance between knowledge and caution, that paves the path towards becoming an AI maestro. Are you ready to stride further along this path? Let's press on!"
                    }
                  ]
                }
              ]
            }
          }
        },
        "topImages": [
          {
            "basename": "ab4-img4",
            "height": 870,
            "width": 870,
            "filename": "ab4-img4.png",
            "format": "png",
            "alt": null,
            "url": "https://www.datocms-assets.com/101962/1690334013-ab4-img4.png"
          }
        ]
      },
      {
        "mainContent": {
          "value": {
            "schema": "dast",
            "document": {
              "type": "root",
              "children": [
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "Potential Pitfalls of LLMs and How to Avoid Them"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Just as mountain hazards can be navigated with the right preparation and knowledge, so too can the pitfalls of LLMs."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "marks": [
                        "strong"
                      ],
                      "value": "What are these pitfalls?"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Firstly, it's easy to underestimate the importance of quality data. Poor data quality is like a faulty compass – it can lead your model astray. Low-quality or biased data can result in skewed outputs, creating models that do more harm than good."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Secondly, the pitfall of over-reliance. It's tempting to sit back and let the model do the work, but remember, LLMs are tools, not autonomous decision-makers. They lack the human touch, the intuitive understanding, the empathic response."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Lastly, the pitfall of misunderstanding the model's limitations. An LLM can't generate new factual knowledge or offer real-time updates. It's like our mountain adventurer using a map to predict tomorrow's weather – it's just not going to work."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "marks": [
                        "strong"
                      ],
                      "value": "So, how do we sidestep these pitfalls?"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Avoid the quality data pitfall by sourcing your data meticulously. Ensure it's representative, unbiased, and as clean as possible. Remember, your model is only as good as the data it learns from."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Bypass over-reliance by adopting a hands-on approach. Guide your LLM, cross-check its outputs, and always apply a layer of human scrutiny. It's your judgement that turns its output from raw data into actionable insights."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Navigate the limitation pitfall by knowing your model's boundaries. Stay updated with AI advancements, read up on the latest research, and keep refining your understanding."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Remember, every great adventurer respects the journey and learns to navigate the risks. The path to AI mastery isn't without its challenges, but with each hurdle crossed, you become a stronger, more competent explorer. So, are you ready to conquer your mountain? Let's press on!\n"
                    }
                  ]
                }
              ]
            }
          }
        },
        "topImages": [
          {
            "basename": "ab4-img5",
            "height": 870,
            "width": 870,
            "filename": "ab4-img5.png",
            "format": "png",
            "alt": null,
            "url": "https://www.datocms-assets.com/101962/1690334091-ab4-img5.png"
          }
        ]
      },
      {
        "mainContent": {
          "value": {
            "schema": "dast",
            "document": {
              "type": "root",
              "children": [
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "Conclusion"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "We've traversed vast expanses in our exploration of Language Learning Models today. From understanding how these impressive models acquire knowledge through diligent training, to demystifying the concept of a knowledge cutoff and recognizing its role in shaping an LLM's performance."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "We've peered into the abyss, confronting the limitations of LLMs, from their struggles with context to their inability to conjure new knowledge. And let's not forget those cunning pitfalls that lurk in the shadows of implementation, which can be safely navigated by applying thoughtful strategies."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Yes, the landscape of AI is vast, and at times, daunting. But remember, every peak surmounted on this journey strengthens your resolve and broadens your perspective. As you look out across the horizon of AI, take a deep breath, and step forward boldly."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "You are a pioneer, an adventurer, a lifelong learner. The challenges you'll face are not roadblocks, but stepping stones on your path to mastery. Keep exploring, keep learning, and most importantly, keep climbing. Because from the peak of this AI mountain, the view is simply unparalleled."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "So, are you ready for your next climb?"
                    }
                  ]
                }
              ]
            }
          }
        },
        "topImages": [
          {
            "basename": "ab4-img6",
            "height": 870,
            "width": 870,
            "filename": "ab4-img6.png",
            "format": "png",
            "alt": null,
            "url": "https://www.datocms-assets.com/101962/1690334121-ab4-img6.png"
          }
        ]
      }
    ],
    "seo": {
      "description": "Understand how they acquire knowledge, their cutoffs, limits, and the potential pitfalls on your AI journey.\"",
      "title": "Decoding the Mysteries of AI",
      "twitterCard": null,
      "image": {
        "width": 870,
        "height": 870,
        "title": null,
        "alt": null,
        "url": "https://www.datocms-assets.com/101962/1690333798-ab4-img1.png"
      }
    }
  },
  "topics": [
    "Getting Started",
    "LLM"
  ]
}