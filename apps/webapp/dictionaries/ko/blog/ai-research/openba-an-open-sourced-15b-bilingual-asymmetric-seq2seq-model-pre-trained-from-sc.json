{
  "relatedBlogs": [],
  "blogContent": {
    "id": "198277342",
    "topics": [
      "LLM",
      "다국어"
    ],
    "title": "OpenBA: 15B 이중 언어 비대칭 seq2seq 모델 오픈 소스 프로젝트",
    "slug": "openba-an-open-sourced-15b-bilingual-asymmetric-seq2seq-model-pre-trained-from-sc",
    "authorName": "Prof. Otto Nomos",
    "authorPicture": {
      "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
    },
    "_publishedAt": "2024-05-27T03:34:02+01:00",
    "description": "추상적 코멘터리 및 평가",
    "thumbnail": {
      "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
    },
    "contentBlock": [
      {
        "mainContent": {
          "value": {
            "schema": "dast",
            "document": {
              "type": "root",
              "children": [
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "2019년 9월 19일 발행"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "저자:"
                    },
                    {
                      "url": "https://huggingface.co/ljtsuda",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "리준타오"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ","
                    },
                    {
                      "url": "https://huggingface.co/ZetangForward",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "탕제청"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ",유양딩,"
                    },
                    {
                      "url": "https://huggingface.co/PinzhengWang",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "왕핀정"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ",구페이,"
                    },
                    {
                      "url": "https://huggingface.co/Moriarty0923",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "유왕지"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ","
                    },
                    {
                      "url": "https://huggingface.co/jorjordan",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "초단"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ","
                    },
                    {
                      "url": "https://huggingface.co/jokephp",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "천원량"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ",푸궈홍,주차오밍,주궈동,장민"
                    }
                  ]
                },
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "초록"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "수십억 개의 매개변수를 가진 대형 언어 모델(LLMs)은 다양한 자연어 처리 작업에서 뛰어난 성능을 보여주었습니다. 이 보고서는 중국 중심의 오픈 소스 모델 커뮤니티에 기여하기 위해 15B 이중 언어 비대칭 seq2seq 모델인 OpenBA를 소개합니다. 우리는 OpenBA를 효과적이고 효율적인 기술로 강화하고 모델을 처음부터 훈련하기 위해 3단계 훈련 전략을 채택했습니다. 우리의 솔루션은 또한 단 380B 토큰으로도 매우 경쟁력 있는 성능을 달성할 수 있으며, 이는 BELEBELE 벤치마크에서 LLaMA-70B보다, MMLU 벤치마크에서 BLOOM-176B보다, C-Eval (hard) 벤치마크에서 GLM-130B보다 우수합니다. 이 보고서는 유사한 모델을 사전 훈련하기 위한 주요 세부 사항을 제공합니다, 사전 훈련 데이터 처리, 이중 언어 Flan 데이터 수집, 모델 아키텍처 설계를 영감을 주는 경험적 관찰, 다양한 단계의 훈련 목표 및 기타 강화 기술을 포함합니다. 우리는 코드를 리팩토링하여 Huggingface Transformers 라이브러리의 설계 원칙을 따르도록 했으며, 개발자가 사용하기 더 편리하게 만들고 https://huggingface.co/openBA에서 다양한 훈련 단계의 체크포인트를 공개했습니다. 우리 프로젝트의 더 많은 세부 사항은 https://github.com/OpenNLG/openBA.git에서 확인할 수 있습니다."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "url": "https://arxiv.org/abs/2309.10706",
                      "meta": [
                        {
                          "id": "rel",
                          "value": "noreferrer"
                        },
                        {
                          "id": "target",
                          "value": "_blank"
                        }
                      ],
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "arXiv 페이지 보기"
                        }
                      ]
                    },
                    {
                      "url": "https://arxiv.org/pdf/2309.10706",
                      "meta": [
                        {
                          "id": "rel",
                          "value": "noreferrer"
                        },
                        {
                          "id": "target",
                          "value": "_blank"
                        }
                      ],
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "PDF 보기"
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "코멘터리"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "\"OpenBA: 15B 이중 언어 비대칭 seq2seq 모델 오픈 소스 프로젝트\" 논문은 중국 중심의 응용 프로그램에 맞춤화된 이중 언어 대형 언어 모델을 소개합니다."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "주요 요약:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "numbered",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "이중 언어 모델"
                            },
                            {
                              "type": "span",
                              "value": ": OpenBA는 중국 중심의 작업을 위해 이중 언어 모델을 제공하여 LLM 커뮤니티에서의 격차를 메웁니다."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "효율적인 기술 및 훈련 전략"
                            },
                            {
                              "type": "span",
                              "value": ": 저자들은 모델을 처음부터 훈련하기 위해 효율적인 기술과 3단계 훈련 전략을 사용합니다. 이는 모델이 적은 토큰으로 훈련되었음에도 불구하고 경쟁력을 유지하도록 보장합니다."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "경쟁력 있는 성능"
                            },
                            {
                              "type": "span",
                              "value": ": OpenBA는 다른 최첨단 모델들과 비교하여 적은 토큰으로 여러 벤치마크에서 더 나은 성능을 달성합니다. 이는 효율적인 아키텍처와 훈련 전략을 시사합니다."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "오픈 소스"
                            },
                            {
                              "type": "span",
                              "value": ": 저자들은 모델을 오픈 소스로 만들고 Huggingface Transformers 라이브러리와 통합했습니다. 이는 개발자와 연구자들이 더 쉽게 채택할 수 있게 합니다."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "잠재적 실제 세계 영향:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "이중 언어 작업"
                            },
                            {
                              "type": "span",
                              "value": ": OpenBA의 이중 언어 특성은 영어와 중국어를 사용하는 다양한 작업에 대응할 수 있어 LLM의 중국어 지역 및 이중 언어 응용 프로그램에서의 적용 가능성을 확장합니다."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "중국 중심 연구 촉진"
                            },
                            {
                              "type": "span",
                              "value": ": 모델의 중국 중심 작업에 초점을 맞춤으로써 이 중요한 언어 그룹을 위한 더 많은 연구 및 응용 프로그램을 장려할 수 있습니다."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "접근 가능한 도구"
                            },
                            {
                              "type": "span",
                              "value": ": 모델이 Huggingface에 통합되고 관련 코드가 오픈 소스로 제공됨에 따라 개발자와 연구자들이 이 모델을 쉽게 채택, 수정 및 확장하여 다양한 응용 프로그램에 사용할 수 있습니다."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "벤치마크 성능"
                            },
                            {
                              "type": "span",
                              "value": ": 벤치마크에서의 우수한 성능은 이 모델이 중국어를 포함하는 이중 언어 NLP 작업에서 표준 또는 참조가 될 가능성을 시사합니다."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "도전:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "특수화된 성격"
                            },
                            {
                              "type": "span",
                              "value": ": 모델은 중국어를 포함하는 이중 언어 작업에 강력하지만, 그 전문성은 다른 언어에서의 더 넓은 적용 가능성을 제한할 수 있습니다."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "자원 집약성"
                            },
                            {
                              "type": "span",
                              "value": ": 다른 대형 모델들과 마찬가지로, 실시간 응용 프로그램이나 자원이 제한된 환경에서의 배치는 도전을 겪을 수 있습니다."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "중국어를 포함하는 이중 언어 작업에서의 혁신 가능성과 오픈 소스 커뮤니티에 대한 기여를 고려할 때:"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "이 논문의 실제 세계 영향을 10점 만점에 8점으로 평가합니다."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "OpenBA는 중국어를 포함하는 이중 언어 작업에 초점을 맞춤으로써 LLM 세계에서 특정한 틈새를 채웁니다. 오픈 소스 성격과 인기 있는 플랫폼과의 통합은 그 채택을 촉진하고 중국 NLP 커뮤니티에서의 추가 연구를 자극할 것입니다."
                    }
                  ]
                }
              ]
            }
          }
        },
        "topImages": [
          {
            "basename": "researchpaper9",
            "height": 816,
            "width": 1456,
            "filename": "researchpaper9.png",
            "format": "png",
            "alt": null,
            "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
          }
        ]
      }
    ],
    "seo": {
      "description": "추상적 코멘터리 및 평가",
      "title": "OpenBA: 15B 이중 언어 ... 모델 ...",
      "twitterCard": null,
      "image": {
        "width": 1456,
        "height": 816,
        "title": null,
        "alt": null,
        "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
      }
    }
  },
  "topics": [
    "LLM",
    "Multilingual"
  ]
}