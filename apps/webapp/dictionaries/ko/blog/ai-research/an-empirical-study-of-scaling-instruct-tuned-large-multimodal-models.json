{
  "relatedBlogs": [],
  "blogContent": {
    "id": "198277150",
    "topics": [
      "LLM",
      "지시 조정",
      "다중 모드"
    ],
    "title": "실증 연구: 규모 조정 지시 조정된 대형 다중 모드 모델",
    "slug": "an-empirical-study-of-scaling-instruct-tuned-large-multimodal-models",
    "authorName": "Prof. Otto Nomos",
    "authorPicture": {
      "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
    },
    "_publishedAt": "2024-05-27T03:33:15+01:00",
    "description": "초록 코멘터리 및 평가",
    "thumbnail": {
      "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
    },
    "contentBlock": [
      {
        "mainContent": {
          "value": {
            "schema": "dast",
            "document": {
              "type": "root",
              "children": [
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "게시일: 9월 18일"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "저자:"
                    },
                    {
                      "url": "https://huggingface.co/Adong17",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "윤동 루"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ","
                    },
                    {
                      "url": "https://huggingface.co/Chunyuan24",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "천원 리"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ","
                    },
                    {
                      "url": "https://huggingface.co/liuhaotian",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "하오텐 류"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ","
                    },
                    {
                      "url": "https://huggingface.co/jw2yang",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "지안웨이 양"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ","
                    },
                    {
                      "url": "https://huggingface.co/wyngjf",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "지안펑 가오"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ","
                    },
                    {
                      "url": "https://huggingface.co/uuu6",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "예롱 신"
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "초록"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "최근 오픈 소스 대형 다중 모드 모델(LMM)인 LLaVA와 MiniGPT-4를 사용한 시각적 지시 조정이 진전을 보이고 있습니다. 그러나 대부분의 기존 연구는 13B 매개변수 이하의 모델을 사용하여 수행됩니다. 이 논문에서는 LLaVA를 33B 및 65B/70B로 확장하는 실증 연구를 제시하고 이미지 해상도, 데이터 혼합 및 LoRA/QLoRA와 같은 매개변수 효율적인 훈련 방법에 대한 탐구 결과를 공유합니다. 이러한 방법은 실제 세계 작업을 완료할 때 다중 모드 및 언어 기능에 미치는 영향을 평가합니다. LMM의 규모를 확장하면 모델 성능이 일관되게 향상되고 언어 기능이 개선되며 LMM의 LoRA/QLoRA 조정 성능은 전체 모델 미세 조정의 성능과 비교할 수 있습니다. 또한, 연구는 더 높은 이미지 해상도와 다중 모드-언어 데이터 혼합의 중요성을 강조하며 때로는 시각적 지시 조정이 LMM의 순수 언어 기능을 향상시킬 수 있습니다. 이 연구가 보다 광범위한 규모에서 최첨단 LMM 연구를 더 접근하기 쉽게 만들어 미래 연구를 위한 강력한 기준을 설정하는 데 도움이 되기를 바랍니다. 코드와 체크포인트는 공개될 예정입니다."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "url": "https://arxiv.org/abs/2309.09958",
                      "meta": [
                        {
                          "id": "rel",
                          "value": "noreferrer"
                        },
                        {
                          "id": "target",
                          "value": "_blank"
                        }
                      ],
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "arXiv 페이지 보기"
                        }
                      ]
                    },
                    {
                      "url": "https://arxiv.org/pdf/2309.09958",
                      "meta": [
                        {
                          "id": "rel",
                          "value": "noreferrer"
                        },
                        {
                          "id": "target",
                          "value": "_blank"
                        }
                      ],
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "PDF 보기"
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "코멘터리"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "논문 \"실증 연구: 규모 조정 지시 조정된 대형 다중 모드 모델\"은 대형 다중 모드 모델(LMM)의 규모 조정의 함의를 이해하는 격차를 메우려고 합니다. 이 모델들은 텍스트와 이미지를 모두 처리하도록 설계되어 다중 모드 이해가 필요한 작업에 매우 유용합니다."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "주요 요약:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "numbered",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "13B 이상 규모 확장"
                            },
                            {
                              "type": "span",
                              "value": ": 이전 연구는 주로 13B 매개변수까지의 오픈 소스 LMM에 중점을 두었습니다. 이 연구는 LLaVA를 33B 및 65B/70B까지 확장하여 더 큰 모델의 역학에 대한 통찰력을 제공합니다."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "탐색 영역"
                            },
                            {
                              "type": "span",
                              "value": ": 연구는 이미지 해상도, 데이터 혼합 및 LoRA/QLoRA와 같은 매개변수 효율적인 훈련 방법과 같은 측면에 중점을 두어 모델 성능에 미치는 영향을 밝힙니다."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "규모 확장의 일관된 이점"
                            },
                            {
                              "type": "span",
                              "value": ": 연구에 따르면 LMM 크기의 규모 확장은 성능을 일관되게 향상시킵니다. 또한 LoRA/QLoRA와 같은 특정 훈련 기술은 전체 모델 미세 조정과 비슷한 성능을 달성할 수 있지만 매개변수 효율성이 더 좋습니다."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "해상도 및 데이터 혼합"
                            },
                            {
                              "type": "span",
                              "value": ": 연구는 더 높은 이미지 해상도와 다중 모드-언어 데이터 혼합의 필요성을 강조하여 LMM의 성능을 향상시키는 데 도움이 됩니다."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "언어 기능 향상"
                            },
                            {
                              "type": "span",
                              "value": ": 흥미롭게도 시각적 지시 조정은 때로는 LMM의 순수 언어 기능을 향상시킬 수 있으며, 이는 다중 모드 학습의 상호 연관된 성격을 더욱 강조합니다."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "잠재적 실제 세계 영향:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "다양한 응용 프로그램"
                            },
                            {
                              "type": "span",
                              "value": ": 텍스트와 이미지를 모두 처리할 수 있는 LMM은 시각적 질문 응답, 이미지 캡션, 콘텐츠 조정 등 다양한 응용 프로그램에 배치될 수 있습니다."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "성능 향상"
                            },
                            {
                              "type": "span",
                              "value": ": 조직과 연구자는 LMM의 규모를 확장함으로써 향상된 성능의 이점을 누릴 수 있으며, 이는 실제 작업에서 더 정확한 결과를 가져올 수 있습니다."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "비용 효율적인 미세 조정"
                            },
                            {
                              "type": "span",
                              "value": ": LoRA/QLoRA와 같은 기술을 사용하면 연구자들이 전체 모델 미세 조정의 부담 없이 높은 성능을 달성할 수 있어 비용과 시간을 절약할 수 있습니다."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "더 나은 이미지 분석"
                            },
                            {
                              "type": "span",
                              "value": ": 고해상도 이미지 입력에 대한 강조는 의료 영상부터 위성 영상 분석에 이르기까지 다양한 분야에서 시각 데이터 처리의 품질을 향상시킬 수 있습니다."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "오픈 사이언스 촉진"
                            },
                            {
                              "type": "span",
                              "value": ": 저자들이 코드와 체크포인트를 공개할 의도는 더 넓은 AI 커뮤니티가 실험, 복제 및 발견을 향상시킬 수 있도록 장려합니다."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "도전 과제:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "계산 제약"
                            },
                            {
                              "type": "span",
                              "value": ": 65B/70B와 같은 큰 모델로 확장하려면 상당한 계산 자원이 필요하며, 많은 연구자와 개발자에게 접근할 수 없을 수 있습니다."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "실제 시나리오로의 전환"
                            },
                            {
                              "type": "span",
                              "value": ": 연구는 견고한 기준을 제공하지만, 특정 산업에서의 실제 배치는 도메인별 조정이 필요할 수 있습니다."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "전자 상거래부터 의료에 이르기까지 다양한 응용 프로그램에서 다중 모드 이해에 대한 강조가 증가함에 따라:"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "이 논문의 실제 세계 영향을 10점 만점에 9점으로 평가합니다."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "이 논문이 제공하는 통찰력은 미래의 다중 모드 시스템의 설계, 훈련 및 배치에 상당한 영향을 미칠 수 있으며, 다양한 분야에서 더 직관적이고 효율적인 인간-AI 상호 작용을 위한 길을 열어줄 것입니다."
                    }
                  ]
                }
              ]
            }
          }
        },
        "topImages": [
          {
            "basename": "researchpaper9",
            "height": 816,
            "width": 1456,
            "filename": "researchpaper9.png",
            "format": "png",
            "alt": null,
            "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
          }
        ]
      }
    ],
    "seo": {
      "description": "초록 코멘터리 및 평가",
      "title": "실증 연구: 규모 조정 지시 조정된 LLM",
      "twitterCard": null,
      "image": {
        "width": 1456,
        "height": 816,
        "title": null,
        "alt": null,
        "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
      }
    }
  },
  "topics": [
    "LLM",
    "Instruction Tuning",
    "Multimodal"
  ]
}