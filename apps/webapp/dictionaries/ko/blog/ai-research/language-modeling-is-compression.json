{
  "relatedBlogs": [],
  "blogContent": {
    "id": "198277277",
    "topics": [
      "LLM",
      "압축"
    ],
    "title": "언어 모델링은 압축입니다",
    "slug": "language-modeling-is-compression",
    "authorName": "Prof. Otto Nomos",
    "authorPicture": {
      "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
    },
    "_publishedAt": "2024-05-25T03:37:32+01:00",
    "description": "추상적인 해설 및 평가",
    "thumbnail": {
      "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
    },
    "contentBlock": [
      {
        "mainContent": {
          "value": {
            "schema": "dast",
            "document": {
              "type": "root",
              "children": [
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "2019년 9월 19일에 게시됨"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "저자:그레고리 델레탕, 아니안 루오스,"
                    },
                    {
                      "url": "https://huggingface.co/padqn",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "폴-암브루아즈 뒤크엔"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ",엘리엇 캇,팀 제네바인,크리스토퍼 매턴,조르디 그라우-모야,리 케빈 웬리앙,매튜 에이치슨,"
                    },
                    {
                      "url": "https://huggingface.co/lorseau",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "로랑 오르소"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ",마커스 허터,조엘 베네스"
                    }
                  ]
                },
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "초록"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "예측 모델을 손실 없는 압축기로 변환할 수 있으며 그 반대의 경우도 마찬가지라는 사실은 오래전부터 입증되었습니다. 우연히 최근 몇 년 동안 기계 학습 커뮤니티는 점점 더 크고 강력한 자기 감독(언어) 모델을 훈련하는 데 집중해 왔습니다. 이러한 대규모 언어 모델은 인상적인 예측 능력을 보여주므로 강력한 압축기가 될 수 있는 좋은 위치에 있습니다. 이 작업에서는 예측 문제를 압축의 관점에서 보고 대규모(기반) 모델의 압축 능력을 평가합니다. 우리는 대규모 언어 모델이 강력한 범용 예측기임을 보여주고 압축 관점이 스케일링 법칙, 토큰화 및 맥락 내 학습에 대한 새로운 통찰력을 제공한다는 것을 보여줍니다. 예를 들어, 주로 텍스트를 기반으로 훈련된 친칠라 70B는 ImageNet 패치를 43.4%로, LibriSpeech 샘플을 16.4%의 원시 크기로 압축하여 각각 PNG(58.5%)나 FLAC(30.3%)과 같은 도메인별 압축기보다 우수합니다. 마지막으로, 예측-압축 등가성은 우리가 gzip과 같은 압축기를 사용하여 조건부 생성 모델을 구축할 수 있게 해줍니다."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "url": "https://arxiv.org/abs/2309.10668",
                      "meta": [
                        {
                          "id": "rel",
                          "value": "noreferrer"
                        },
                        {
                          "id": "target",
                          "value": "_blank"
                        }
                      ],
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "arXiv 페이지 보기"
                        }
                      ]
                    },
                    {
                      "url": "https://arxiv.org/pdf/2309.10668",
                      "meta": [
                        {
                          "id": "rel",
                          "value": "noreferrer"
                        },
                        {
                          "id": "target",
                          "value": "_blank"
                        }
                      ],
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "PDF 보기"
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "해설"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "\"언어 모델링은 압축입니다\"라는 논문은 예측 모델도 손실 없는 압축기로 활용될 수 있다는 잘 알려진 개념을 다시 검토하고 이 아이디어가 현대의 대규모 언어 모델에 어떻게 적용될 수 있는지 평가합니다."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "주요 요약:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "numbered",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "압축으로서의 예측"
                            },
                            {
                              "type": "span",
                              "value": ": 이 논문은 예측 모델(현대 LLM과 같은)을 효율적인 압축기로 사용하는 아이디어를 밀어붙입니다. 강력한 예측 능력을 가진 이 모델들은 다양한 데이터 유형을 압축할 수 있습니다."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "범용 압축기"
                            },
                            {
                              "type": "span",
                              "value": ": 연구는 대규모 언어 모델이 주로 텍스트를 기반으로 훈련되었음에도 불구하고 비텍스트 데이터를 효율적으로 압축할 수 있음을 나타냅니다. 예를 들어, 친칠라 70B는 ImageNet 이미지와 LibriSpeech 오디오 샘플을 PNG와 FLAC과 같은 도메인별 압축기보다 더 잘 압축할 수 있습니다."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "통찰력 획득"
                            },
                            {
                              "type": "span",
                              "value": ": 예측을 압축 관점에서 보는 것은 기계 학습의 다양한 측면, 예를 들어 스케일링 법칙, 토큰화 및 맥락 내 학습에 대한 통찰력을 제공할 수 있습니다."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "압축기에서 생성 모델까지"
                            },
                            {
                              "type": "span",
                              "value": ": 예측과 압축의 등가성은 gzip과 같은 압축기를 사용하여 조건부 생성 모델을 만드는 데 사용될 수 있습니다."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "잠재적인 실제 세계 영향:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "데이터 저장 및 전송"
                            },
                            {
                              "type": "span",
                              "value": ": LLM이 압축기로 효과적으로 사용될 수 있다면, 특히 이미지와 오디오와 같은 풍부한 미디어의 데이터 저장 및 전송을 혁신할 수 있습니다."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "텍스트를 넘어서"
                            },
                            {
                              "type": "span",
                              "value": ": 텍스트로 훈련된 모델이 비텍스트 데이터를 압축할 수 있음을 보여주는 것은 멀티 모달 애플리케이션에 대한 문을 열고 현대 LLM의 일반화 능력을 보여줍니다."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "LLM에 대한 더 나은 이해"
                            },
                            {
                              "type": "span",
                              "value": ": 압축 관점은 대규모 언어 모델의 기능 및 잠재적 응용 프로그램에 대한 더 깊은 통찰력을 제공할 수 있습니다."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "생성 애플리케이션"
                            },
                            {
                              "type": "span",
                              "value": ": 어떤 압축기든 조건부 생성 모델로 변환할 수 있는 능력은 데이터 생성, 합성 및 증강 작업에서 광범위한 영향을 미칠 수 있습니다."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "도전 과제:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "계산 자원"
                            },
                            {
                              "type": "span",
                              "value": ": 대규모 언어 모델을 압축기로 사용하는 것은 계산 비용이 많이 들어 실시간 애플리케이션 또는 리소스가 제한된 사용자에게 접근하기 어려울 수 있습니다."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "도메인 전문 지식"
                            },
                            {
                              "type": "span",
                              "value": ": 일부 특정 도메인의 경우 도메인별 제약 및 요구 사항으로 인해 전문 압축기가 여전히 선호될 수 있습니다."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "LLM의 데이터 저장, 전송 및 더 넓은 이해에 대한 잠재적인 돌파구를 고려할 때:"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "이 논문의 실제 세계 영향을 10점 만점에 9점으로 평가합니다."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "예측과 압축 사이의 연결은 전혀 새로운 것은 아니지만, 이 논문이 현대 LLM에 적용되고 달성한 결과는 주목할 만합니다. 이러한 발견이 효과적으로 구현될 수 있다면, 새로운 애플리케이션과 언어 모델에 대한 더 깊은 이해를 위한 길을 열 수 있습니다."
                    }
                  ]
                }
              ]
            }
          }
        },
        "topImages": [
          {
            "basename": "researchpaper9",
            "height": 816,
            "width": 1456,
            "filename": "researchpaper9.png",
            "format": "png",
            "alt": null,
            "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
          }
        ]
      }
    ],
    "seo": {
      "description": "추상적인 해설 및 평가",
      "title": "언어 모델링은 압축입니다",
      "twitterCard": null,
      "image": {
        "width": 1456,
        "height": 816,
        "title": null,
        "alt": null,
        "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
      }
    }
  },
  "topics": [
    "LLM",
    "Compression"
  ]
}