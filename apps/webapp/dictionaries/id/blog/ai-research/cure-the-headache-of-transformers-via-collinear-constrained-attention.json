{
  "relatedBlogs": [],
  "blogContent": {
    "id": "198277182",
    "topics": [
      "Transformers"
    ],
    "title": "Sembuhkan sakit kepala Transformers melalui Perhatian Terkendala Kolinear",
    "slug": "cure-the-headache-of-transformers-via-collinear-constrained-attention",
    "authorName": "Prof. Otto Nomos",
    "authorPicture": {
      "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
    },
    "_publishedAt": "2024-05-27T03:32:43+01:00",
    "description": "Komentar & Penilaian Abstrak",
    "thumbnail": {
      "url": "https://www.datocms-assets.com/101962/1692841427-researchpaper8a.png"
    },
    "contentBlock": [
      {
        "mainContent": {
          "value": {
            "schema": "dast",
            "document": {
              "type": "root",
              "children": [
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Diterbitkan pada 15 Sep"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Penulis:"
                    },
                    {
                      "url": "https://huggingface.co/underskies",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "Shiyi Zhu"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ",Jing Ye,Wei Jiang,Qi Zhang,Yifan Wu,"
                    },
                    {
                      "url": "https://huggingface.co/JungleLee",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "Jianguo Li"
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "Abstrak"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Seiring dengan kemajuan pesat aplikasi praktis yang berbasis Model Bahasa Besar, pentingnya mengekstrapolasi kinerja telah tumbuh secara eksponensial dalam domain penelitian. Dalam studi kami, kami mengidentifikasi perilaku anomali pada model Transformer yang sebelumnya diabaikan, yang menyebabkan kekacauan di sekitar token terdekat yang membawa informasi paling penting. Kami menyebut penemuan ini sebagai \"sakit kepala Transformers\". Untuk mengatasi ini dari akarnya, kami memperkenalkan struktur perhatian diri baru yang disebut Perhatian Terkendala Kolinear (CoCA). Struktur ini dapat diintegrasikan dengan mulus dengan metode ekstrapolasi, interpolasi, dan strategi optimasi lainnya yang dirancang untuk model Transformer tradisional. Kami telah mencapai kinerja ekstrapolasi yang sangat baik bahkan untuk 16 hingga 24 kali panjang urutan selama inferensi tanpa penyetelan ulang pada model kami. Kami juga telah meningkatkan efisiensi komputasi dan spasial CoCA untuk memastikan kepraktisannya. Kami berencana untuk membuka sumber CoCA dalam waktu dekat. Sementara itu, kami telah menyediakan kode kami di lampiran untuk eksperimen ulang."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "url": "https://arxiv.org/abs/2309.08646",
                      "meta": [
                        {
                          "id": "rel",
                          "value": "noreferrer"
                        },
                        {
                          "id": "target",
                          "value": "_blank"
                        }
                      ],
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "Lihat halaman arXiv"
                        }
                      ]
                    },
                    {
                      "url": "https://arxiv.org/pdf/2309.08646",
                      "meta": [
                        {
                          "id": "rel",
                          "value": "noreferrer"
                        },
                        {
                          "id": "target",
                          "value": "_blank"
                        }
                      ],
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "Lihat PDF"
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "Komentar"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Makalah \"Sembuhkan sakit kepala Transformers melalui Perhatian Terkendala Kolinear\" mengidentifikasi dan mengatasi masalah yang diabaikan pada model Transformer, arsitektur dominan dalam berbagai tugas dan aplikasi pemrosesan bahasa alami."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Poin Utama:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "numbered",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Identifikasi Perilaku Anomali"
                            },
                            {
                              "type": "span",
                              "value": ": Penelitian ini mengidentifikasi perilaku yang disebut \"sakit kepala Transformers,\" di mana ada perilaku kacau di sekitar token terdekat, yang sering kali paling informatif. Ini menimbulkan tantangan dalam kinerja, terutama dalam tugas yang memerlukan perhatian pada urutan panjang."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Perhatian Terkendala Kolinear (CoCA)"
                            },
                            {
                              "type": "span",
                              "value": ": Penulis memperkenalkan struktur perhatian diri baru untuk mengatasi masalah ini, yang diklaim mudah diintegrasikan dengan metode optimasi lainnya untuk model Transformer tradisional."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Ekstrapolasi Superior"
                            },
                            {
                              "type": "span",
                              "value": ": Makalah ini menyarankan bahwa dengan CoCA, model dapat melakukan ekstrapolasi secara efisien pada panjang urutan yang panjang tanpa memerlukan penyetelan ulang tambahan."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Peningkatan Efisiensi"
                            },
                            {
                              "type": "span",
                              "value": ": Selain peningkatan akurasi, para peneliti telah mengoptimalkan CoCA untuk efisiensi komputasi dan spasial, yang membuatnya lebih praktis untuk penerapan di dunia nyata."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Sumber Terbuka"
                            },
                            {
                              "type": "span",
                              "value": ": Para peneliti menyatakan niat untuk membuat CoCA sumber terbuka, yang kemungkinan akan mendorong adopsi dan eksplorasi lebih lanjut oleh komunitas NLP yang lebih luas."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Dampak Potensial di Dunia Nyata:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Perilaku Model yang Lebih Baik"
                            },
                            {
                              "type": "span",
                              "value": ": Dengan mengatasi masalah mendasar pada Transformers, model mungkin lebih stabil dan dapat diprediksi, yang mengarah pada kinerja dunia nyata yang lebih baik, terutama dalam tugas di mana pemahaman konteks pada urutan panjang sangat penting."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Pemrosesan Urutan Panjang yang Efisien"
                            },
                            {
                              "type": "span",
                              "value": ": Mengingat peningkatan ekstrapolasi, tugas seperti ringkasan dokumen, yang memerlukan perhatian pada teks yang lebih panjang, dapat memperoleh manfaat."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Integrasi Umum"
                            },
                            {
                              "type": "span",
                              "value": ": Kemudahan integrasi dengan metode optimasi lainnya berarti bahwa berbagai model Transformer yang ada dapat memperoleh manfaat tanpa perombakan total."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Tantangan:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Tingkat Adopsi"
                            },
                            {
                              "type": "span",
                              "value": ": Seperti teknik baru lainnya, mungkin perlu waktu bagi komunitas yang lebih luas untuk mengadopsi, menguji, dan memvalidasi pendekatan ini dalam berbagai skenario dunia nyata."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Potensi Keterbatasan"
                            },
                            {
                              "type": "span",
                              "value": ": Setiap model atau teknik memiliki keterbatasannya, yang mungkin hanya menjadi jelas setelah diterapkan pada berbagai tugas."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Mengingat potensi manfaat dari mengatasi tantangan mendasar pada Transformers, bersama dengan keuntungan praktis yang tampaknya ditawarkan oleh pendekatan ini:"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Saya akan menilai dampak dunia nyata dari makalah ini sebagai 8 dari 10."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Peningkatan efisiensi dan kinerja yang potensial di berbagai tugas dapat memiliki implikasi signifikan untuk banyak aplikasi NLP. Namun, dampak penuh akan sangat bergantung pada bagaimana komunitas penelitian dan pengembangan yang lebih luas menerima, memvalidasi, dan mengimplementasikan temuan ini."
                    }
                  ]
                }
              ]
            }
          }
        },
        "topImages": [
          {
            "basename": "researchpaper8a",
            "height": 816,
            "width": 1456,
            "filename": "researchpaper8a.png",
            "format": "png",
            "alt": null,
            "url": "https://www.datocms-assets.com/101962/1692841427-researchpaper8a.png"
          }
        ]
      }
    ],
    "seo": {
      "description": "Komentar & Penilaian Abstrak\n",
      "title": "Sembuhkan sakit kepala ... melalui Perhatian Terkendala Kolinear",
      "twitterCard": null,
      "image": {
        "width": 1456,
        "height": 816,
        "title": null,
        "alt": null,
        "url": "https://www.datocms-assets.com/101962/1692841427-researchpaper8a.png"
      }
    }
  },
  "topics": [
    "Transformers"
  ]
}