{
  "relatedBlogs": [],
  "blogContent": {
    "id": "198277182",
    "topics": [
      "Transformadores"
    ],
    "title": "Cure a dor de cabeça dos Transformadores via Atenção Restrita Colinear",
    "slug": "cure-the-headache-of-transformers-via-collinear-constrained-attention",
    "authorName": "Prof. Otto Nomos",
    "authorPicture": {
      "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
    },
    "_publishedAt": "2024-05-27T03:32:43+01:00",
    "description": "Comentário e Avaliação Abstrata",
    "thumbnail": {
      "url": "https://www.datocms-assets.com/101962/1692841427-researchpaper8a.png"
    },
    "contentBlock": [
      {
        "mainContent": {
          "value": {
            "schema": "dast",
            "document": {
              "type": "root",
              "children": [
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Publicado em 15 de Setembro"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Autores:"
                    },
                    {
                      "url": "https://huggingface.co/underskies",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "Shiyi Zhu"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ",Jing Ye,Wei Jiang,Qi Zhang,Yifan Wu,"
                    },
                    {
                      "url": "https://huggingface.co/JungleLee",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "Jianguo Li"
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "Resumo"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "À medida que a rápida progressão de aplicações práticas baseadas em Modelos de Grandes Linguagens continua, a importância de extrapolar o desempenho cresceu exponencialmente no domínio da pesquisa. Em nosso estudo, identificamos um comportamento anômalo em modelos de Transformador que havia sido anteriormente ignorado, levando a um caos em torno dos tokens mais próximos que carregavam as informações mais importantes. Nós denominamos essa descoberta a \"dor de cabeça dos Transformadores\". Para abordar isso em sua essência, introduzimos uma nova estrutura de autoatenção chamada Atenção Restrita Colinear (CoCA). Esta estrutura pode ser integrada de forma contínua com métodos de extrapolação e interpolação existentes, e outras estratégias de otimização projetadas para modelos de Transformador tradicionais. Alcançamos um desempenho de extrapolação excelente mesmo para comprimentos de sequência de 16 a 24 vezes durante a inferência sem qualquer ajuste fino em nosso modelo. Também melhoramos a eficiência computacional e espacial do CoCA para garantir sua praticidade. Planejamos disponibilizar o CoCA em código aberto em breve. Enquanto isso, disponibilizamos nosso código no apêndice para reprodução de experimentos."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "url": "https://arxiv.org/abs/2309.08646",
                      "meta": [
                        {
                          "id": "rel",
                          "value": "noreferrer"
                        },
                        {
                          "id": "target",
                          "value": "_blank"
                        }
                      ],
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "Ver página arXiv"
                        }
                      ]
                    },
                    {
                      "url": "https://arxiv.org/pdf/2309.08646",
                      "meta": [
                        {
                          "id": "rel",
                          "value": "noreferrer"
                        },
                        {
                          "id": "target",
                          "value": "_blank"
                        }
                      ],
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "Ver PDF"
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "Comentário"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "O artigo \"Cure a dor de cabeça dos Transformadores via Atenção Restrita Colinear\" identifica e aborda uma questão negligenciada em modelos de Transformador, uma arquitetura dominante em várias tarefas e aplicações de processamento de linguagem natural."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Principais Conclusões:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "numbered",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Identificação de Comportamento Anômalo"
                            },
                            {
                              "type": "span",
                              "value": ": A pesquisa identifica um comportamento denominado \"dor de cabeça dos Transformadores,\" onde há comportamento caótico em torno dos tokens mais próximos, que são frequentemente os mais informativos. Isso apresenta desafios no desempenho, especialmente em tarefas que exigem atenção ao longo de sequências longas."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Atenção Restrita Colinear (CoCA)"
                            },
                            {
                              "type": "span",
                              "value": ": Os autores introduzem uma nova estrutura de autoatenção para abordar essa questão, que é afirmada ser facilmente integrada com outros métodos de otimização para modelos de Transformador tradicionais."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Extrapolação Superior"
                            },
                            {
                              "type": "span",
                              "value": ": O artigo sugere que com o CoCA, os modelos podem realizar extrapolação de forma eficiente ao longo de comprimentos de sequência longos sem necessidade de ajuste fino adicional."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Melhorias de Eficiência"
                            },
                            {
                              "type": "span",
                              "value": ": Além das melhorias de precisão, os pesquisadores otimizaram o CoCA para eficiência computacional e espacial, o que o torna mais prático para implantações no mundo real."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Código Aberto"
                            },
                            {
                              "type": "span",
                              "value": ": Os pesquisadores expressam a intenção de tornar o CoCA de código aberto, o que provavelmente incentivará a adoção e exploração adicional pela comunidade de PLN mais ampla."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Impacto Potencial no Mundo Real:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Melhor Comportamento do Modelo"
                            },
                            {
                              "type": "span",
                              "value": ": Ao abordar uma questão subjacente nos Transformadores, os modelos podem ser mais estáveis e previsíveis, levando a um melhor desempenho no mundo real, especialmente em tarefas onde entender o contexto ao longo de sequências longas é crucial."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Processamento Eficiente de Sequências Longas"
                            },
                            {
                              "type": "span",
                              "value": ": Dadas as melhorias de extrapolação, tarefas como resumo de documentos, que exigem atenção sobre textos mais longos, poderiam se beneficiar."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Integração Geral"
                            },
                            {
                              "type": "span",
                              "value": ": A facilidade de integração com outros métodos de otimização significa que uma ampla gama de modelos de Transformador existentes pode se beneficiar sem revisões completas."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Desafios:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Taxa de Adoção"
                            },
                            {
                              "type": "span",
                              "value": ": Como qualquer técnica nova, pode levar tempo para que a comunidade mais ampla adote, teste e valide a abordagem em cenários do mundo real diversos."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Limitações Potenciais"
                            },
                            {
                              "type": "span",
                              "value": ": Todo modelo ou técnica tem suas limitações, que podem se tornar evidentes apenas quando aplicadas a uma variedade maior de tarefas."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Dado os benefícios potenciais de abordar um desafio fundamental nos Transformadores, junto com as vantagens práticas que a abordagem parece oferecer:"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Eu classificaria o impacto real deste artigo como 8 de 10."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "As melhorias potenciais em eficiência e desempenho em uma ampla gama de tarefas podem ter implicações significativas para numerosas aplicações de PLN. No entanto, o impacto completo dependerá em grande parte de como a comunidade mais ampla de pesquisa e desenvolvimento recebe, valida e implementa as descobertas."
                    }
                  ]
                }
              ]
            }
          }
        },
        "topImages": [
          {
            "basename": "researchpaper8a",
            "height": 816,
            "width": 1456,
            "filename": "researchpaper8a.png",
            "format": "png",
            "alt": null,
            "url": "https://www.datocms-assets.com/101962/1692841427-researchpaper8a.png"
          }
        ]
      }
    ],
    "seo": {
      "description": "Comentário e Avaliação Abstrata\n",
      "title": "Cure a dor de cabeça ... via Atenção Restrita Colinear",
      "twitterCard": null,
      "image": {
        "width": 1456,
        "height": 816,
        "title": null,
        "alt": null,
        "url": "https://www.datocms-assets.com/101962/1692841427-researchpaper8a.png"
      }
    }
  },
  "topics": [
    "Transformers"
  ]
}