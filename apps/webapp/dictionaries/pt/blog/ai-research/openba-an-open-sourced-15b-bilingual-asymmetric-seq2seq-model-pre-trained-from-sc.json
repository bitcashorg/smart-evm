{
  "relatedBlogs": [],
  "blogContent": {
    "id": "198277342",
    "topics": [
      "LLM",
      "Multilíngue"
    ],
    "title": "OpenBA: Um Modelo Sequencial Assimetrico Bilíngue de 15B de Código Aberto Pre-treinado do Zero",
    "slug": "openba-an-open-sourced-15b-bilingual-asymmetric-seq2seq-model-pre-trained-from-sc",
    "authorName": "Prof. Otto Nomos",
    "authorPicture": {
      "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
    },
    "_publishedAt": "2024-05-27T03:34:02+01:00",
    "description": "Comentário e Avaliação do Resumo",
    "thumbnail": {
      "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
    },
    "contentBlock": [
      {
        "mainContent": {
          "value": {
            "schema": "dast",
            "document": {
              "type": "root",
              "children": [
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Publicado em 19 de Set"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Autores:"
                    },
                    {
                      "url": "https://huggingface.co/ljtsuda",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "Juntao Li"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ","
                    },
                    {
                      "url": "https://huggingface.co/ZetangForward",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "Zecheng Tang"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ",Yuyang Ding,"
                    },
                    {
                      "url": "https://huggingface.co/PinzhengWang",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "Pinzheng Wang"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ",Pei Guo,"
                    },
                    {
                      "url": "https://huggingface.co/Moriarty0923",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "Wangjie You"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ","
                    },
                    {
                      "url": "https://huggingface.co/jorjordan",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "Dan Qiao"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ","
                    },
                    {
                      "url": "https://huggingface.co/jokephp",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "Wenliang Chen"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ",Guohong Fu,Qiaoming Zhu,Guodong Zhou,Min Zhang"
                    }
                  ]
                },
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "Resumo"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Modelos de linguagem grandes (LLMs) com bilhões de parâmetros demonstraram desempenho excepcional em várias tarefas de processamento de linguagem natural. Este relatório apresenta o OpenBA, um modelo sequencial assimetrico bilíngue de 15B de código aberto, para contribuir com uma variante de LLM voltada para a comunidade de modelos de código aberto orientada para o chinês. Aprimoramos o OpenBA com técnicas eficazes e eficientes, além de adotar uma estratégia de treinamento em três estágios para treinar o modelo do zero. Nossa solução também pode alcançar um desempenho muito competitivo com apenas 380B de tokens, o que é melhor que o LLaMA-70B no benchmark BELEBELE, BLOOM-176B no benchmark MMLU, GLM-130B no benchmark C-Eval (difícil). Este relatório fornece os principais detalhes para pré-treinar um modelo análogo, incluindo processamento de dados de pré-treinamento, coleta de dados Flan Bilíngue, as observações empíricas que inspiram nosso design de arquitetura de modelo, objetivos de treinamento de diferentes estágios e outras técnicas de aprimoramento. Refatoramos nosso código para seguir os princípios de design da Biblioteca de Transformadores do Huggingface, tornando-o mais conveniente para os desenvolvedores usarem, e lançamos pontos de verificação de diferentes estágios de treinamento em https://huggingface.co/openBA. Mais detalhes do nosso projeto estão disponíveis em https://github.com/OpenNLG/openBA.git."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "url": "https://arxiv.org/abs/2309.10706",
                      "meta": [
                        {
                          "id": "rel",
                          "value": "noreferrer"
                        },
                        {
                          "id": "target",
                          "value": "_blank"
                        }
                      ],
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "Ver página do arXiv"
                        }
                      ]
                    },
                    {
                      "url": "https://arxiv.org/pdf/2309.10706",
                      "meta": [
                        {
                          "id": "rel",
                          "value": "noreferrer"
                        },
                        {
                          "id": "target",
                          "value": "_blank"
                        }
                      ],
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "Ver PDF"
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "Comentário"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "O artigo \"OpenBA: Um Modelo Sequencial Assimetrico Bilíngue de 15B de Código Aberto Pre-treinado do Zero\" apresenta um modelo de linguagem grande bilíngue voltado para aplicações orientadas para o chinês."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Principais Conclusões:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "numbered",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Modelo Bilíngue"
                            },
                            {
                              "type": "span",
                              "value": ": OpenBA é um modelo bilíngue voltado para tarefas orientadas para o chinês, preenchendo assim uma lacuna na comunidade LLM."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Técnicas Eficientes e Estratégia de Treinamento"
                            },
                            {
                              "type": "span",
                              "value": ": Os autores usam técnicas eficientes e uma estratégia de treinamento em três estágios para treinar o modelo do zero. Isso garante que o modelo seja competitivo, apesar de ser treinado com menos tokens em comparação com outros modelos grandes."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Desempenho Competitivo"
                            },
                            {
                              "type": "span",
                              "value": ": O OpenBA alcança um desempenho melhor em vários benchmarks em comparação com outros modelos de última geração, com menos tokens. Isso sugere uma arquitetura e estratégias de treinamento eficientes."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Código Aberto"
                            },
                            {
                              "type": "span",
                              "value": ": Os autores tornaram seu modelo de código aberto e integrado com a Biblioteca de Transformadores do Huggingface. Isso facilita a adoção mais fácil por desenvolvedores e pesquisadores."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Impacto Potencial no Mundo Real:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Tarefas Bilíngues"
                            },
                            {
                              "type": "span",
                              "value": ": A natureza bilíngue do OpenBA pode abordar uma ampla gama de tarefas que envolvem os idiomas inglês e chinês, expandindo a aplicabilidade dos LLMs em regiões de língua chinesa e aplicações bilíngues."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Promoção de Pesquisa Orientada para o Chinês"
                            },
                            {
                              "type": "span",
                              "value": ": O foco do modelo em tarefas orientadas para o chinês pode incentivar mais pesquisas e aplicações voltadas para esse significativo grupo linguístico."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Ferramenta Acessível"
                            },
                            {
                              "type": "span",
                              "value": ": Com o modelo integrado ao Huggingface e o código associado sendo de código aberto, desenvolvedores e pesquisadores podem facilmente adotar, modificar e estender esse modelo para várias aplicações."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Desempenho em Benchmarks"
                            },
                            {
                              "type": "span",
                              "value": ": O desempenho superior em benchmarks sugere a possibilidade deste modelo se tornar um padrão ou referência em tarefas de PNL bilíngue envolvendo chinês."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Desafios:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Natureza Especializada"
                            },
                            {
                              "type": "span",
                              "value": ": Embora o modelo seja poderoso para tarefas bilíngues envolvendo chinês, sua especialização pode limitar sua aplicabilidade mais ampla em outros idiomas."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Intensidade de Recursos"
                            },
                            {
                              "type": "span",
                              "value": ": Como outros grandes modelos, aplicações em tempo real ou implantações em ambientes com recursos limitados podem enfrentar desafios."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Dado o potencial para avanços em tarefas bilíngues envolvendo chinês e sua contribuição para a comunidade de código aberto:"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Eu classificaria o impacto real deste artigo como 8 de 10."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "OpenBA preenche um nicho específico no mundo LLM ao atender a tarefas bilíngues envolvendo chinês. A natureza de código aberto e a integração com plataformas populares provavelmente promoverão sua adoção e estimularão mais pesquisas na comunidade de PNL chinesa."
                    }
                  ]
                }
              ]
            }
          }
        },
        "topImages": [
          {
            "basename": "researchpaper9",
            "height": 816,
            "width": 1456,
            "filename": "researchpaper9.png",
            "format": "png",
            "alt": null,
            "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
          }
        ]
      }
    ],
    "seo": {
      "description": "Comentário e Avaliação do Resumo",
      "title": "OpenBA: Um Modelo Sequencial Assimetrico Bilíngue de 15B de Código Aberto ...",
      "twitterCard": null,
      "image": {
        "width": 1456,
        "height": 816,
        "title": null,
        "alt": null,
        "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
      }
    }
  },
  "topics": [
    "LLM",
    "Multilingual"
  ]
}