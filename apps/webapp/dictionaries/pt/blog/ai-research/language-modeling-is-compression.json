{
  "relatedBlogs": [],
  "blogContent": {
    "id": "198277277",
    "topics": [
      "LLM",
      "Compressão"
    ],
    "title": "Modelagem de Linguagem é Compressão",
    "slug": "language-modeling-is-compression",
    "authorName": "Prof. Otto Nomos",
    "authorPicture": {
      "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
    },
    "_publishedAt": "2024-05-25T03:37:32+01:00",
    "description": "Comentário e Avaliação Abstratos",
    "thumbnail": {
      "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
    },
    "contentBlock": [
      {
        "mainContent": {
          "value": {
            "schema": "dast",
            "document": {
              "type": "root",
              "children": [
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Publicado em 19 de Set"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Autores: Grégoire Delétang, Anian Ruoss,"
                    },
                    {
                      "url": "https://huggingface.co/padqn",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "Paul-Ambroise Duquenne"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ", Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison,"
                    },
                    {
                      "url": "https://huggingface.co/lorseau",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "Laurent Orseau"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ", Marcus Hutter, Joel Veness"
                    }
                  ]
                },
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "Resumo"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Há muito tempo está estabelecido que modelos preditivos podem ser transformados em compressores sem perdas e vice-versa. Incidentalmente, nos últimos anos, a comunidade de aprendizado de máquina tem se concentrado em treinar modelos de auto-supervisão (linguagem) cada vez maiores e mais poderosos. Uma vez que esses grandes modelos de linguagem exibem capacidades preditivas impressionantes, eles estão bem posicionados para serem compressores eficazes. Neste trabalho, defendemos a visão do problema de predição através da lente da compressão e avaliamos as capacidades de compressão de grandes modelos (fundamentais). Mostramos que grandes modelos de linguagem são preditores de propósito geral poderosos e que a perspectiva de compressão fornece novas percepções sobre leis de escalonamento, tokenização e aprendizado no contexto. Por exemplo, Chinchilla 70B, embora treinado principalmente em texto, comprime patches do ImageNet para 43,4% e amostras do LibriSpeech para 16,4% de seu tamanho bruto, superando compressores específicos de domínio como PNG (58,5%) ou FLAC (30,3%), respectivamente. Finalmente, mostramos que a equivalência de predição-compressão nos permite usar qualquer compressor (como gzip) para construir um modelo gerativo condicional."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "url": "https://arxiv.org/abs/2309.10668",
                      "meta": [
                        {
                          "id": "rel",
                          "value": "noreferrer"
                        },
                        {
                          "id": "target",
                          "value": "_blank"
                        }
                      ],
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "Ver página arXiv"
                        }
                      ]
                    },
                    {
                      "url": "https://arxiv.org/pdf/2309.10668",
                      "meta": [
                        {
                          "id": "rel",
                          "value": "noreferrer"
                        },
                        {
                          "id": "target",
                          "value": "_blank"
                        }
                      ],
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "Ver PDF"
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "Comentário"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "O artigo \"Modelagem de Linguagem é Compressão\" revisita o conceito bem estabelecido de que modelos preditivos também podem ser utilizados como compressores sem perdas e avalia como essa ideia pode ser aplicada a modelos de linguagem de grande escala e modernos."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Principais Conclusões:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "numbered",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Compressão como Predição"
                            },
                            {
                              "type": "span",
                              "value": ": O artigo promove a ideia de usar modelos preditivos (como LLMs modernos) como compressores eficientes. Dada a sua forte capacidade preditiva, esses modelos podem comprimir uma ampla variedade de tipos de dados."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Compressores de Propósito Geral"
                            },
                            {
                              "type": "span",
                              "value": ": A pesquisa indica que grandes modelos de linguagem, mesmo treinados principalmente em texto, podem comprimir dados não textuais de forma eficiente. Por exemplo, Chinchilla 70B pode comprimir imagens do ImageNet e amostras de áudio do LibriSpeech melhor do que compressores específicos de domínio como PNG e FLAC."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Ganhando Percepções"
                            },
                            {
                              "type": "span",
                              "value": ": Ver a predição sob a perspectiva de compressão pode fornecer percepções sobre vários aspectos do aprendizado de máquina, como leis de escalonamento, tokenização e aprendizado no contexto."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Modelos Gerativos a partir de Compressores"
                            },
                            {
                              "type": "span",
                              "value": ": A equivalência de predição e compressão possibilita a criação de modelos gerativos condicionais usando qualquer compressor."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Impacto Potencial no Mundo Real:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Armazenamento e Transferência de Dados"
                            },
                            {
                              "type": "span",
                              "value": ": Se LLMs podem ser efetivamente usados como compressores, eles podem revolucionar o armazenamento e transmissão de dados, especialmente para mídias ricas como imagens e áudio."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Além do Texto"
                            },
                            {
                              "type": "span",
                              "value": ": Demonstrar que um modelo treinado em texto pode comprimir dados não textuais abre portas para aplicações multimodais e mostra a capacidade de generalização dos LLMs modernos."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Melhor Compreensão dos LLMs"
                            },
                            {
                              "type": "span",
                              "value": ": A perspectiva de compressão pode fornecer insights mais profundos sobre o funcionamento e as aplicações potenciais de grandes modelos de linguagem."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Aplicações Gerativas"
                            },
                            {
                              "type": "span",
                              "value": ": A capacidade de transformar qualquer compressor em um modelo gerativo condicional pode ter implicações abrangentes em tarefas de geração, síntese e aumento de dados."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Desafios:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Recursos Computacionais"
                            },
                            {
                              "type": "span",
                              "value": ": Usar grandes modelos de linguagem como compressores pode ser computacionalmente caro, tornando-os menos acessíveis para aplicações em tempo real ou para usuários com recursos limitados."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Expertise de Domínio"
                            },
                            {
                              "type": "span",
                              "value": ": Para alguns domínios específicos, compressores especializados ainda podem ser preferidos devido a restrições e requisitos específicos do domínio."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Dado o potencial para avanços no armazenamento de dados, transmissão e uma compreensão mais ampla dos LLMs:"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Eu classificaria o impacto real deste artigo como 9 de 10."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "A ponte entre predição e compressão não é totalmente nova, mas a aplicação do artigo aos LLMs modernos e os resultados alcançados são notáveis. Se essas descobertas puderem ser implementadas de forma eficiente, isso pode abrir caminho para novas aplicações e uma compreensão mais profunda dos modelos de linguagem."
                    }
                  ]
                }
              ]
            }
          }
        },
        "topImages": [
          {
            "basename": "researchpaper9",
            "height": 816,
            "width": 1456,
            "filename": "researchpaper9.png",
            "format": "png",
            "alt": null,
            "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
          }
        ]
      }
    ],
    "seo": {
      "description": "Comentário e Avaliação Abstratos",
      "title": "Modelagem de Linguagem é Compressão",
      "twitterCard": null,
      "image": {
        "width": 1456,
        "height": 816,
        "title": null,
        "alt": null,
        "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
      }
    }
  },
  "topics": [
    "LLM",
    "Compression"
  ]
}