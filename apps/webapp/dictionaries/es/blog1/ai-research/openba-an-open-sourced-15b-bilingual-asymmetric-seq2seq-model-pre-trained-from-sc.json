{
  "relatedBlogs": [],
  "blogContent": {
    "id": "198277342",
    "topics": [
      "LLM",
      "Multilingüe"
    ],
    "title": "OpenBA: Un modelo secuencial asimétrico bilingüe de 15B de código abierto preentrenado desde cero",
    "slug": "openba-an-open-sourced-15b-bilingual-asymmetric-seq2seq-model-pre-trained-from-sc",
    "authorName": "Prof. Otto Nomos",
    "authorPicture": {
      "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
    },
    "_publishedAt": "2024-05-27T03:34:02+01:00",
    "description": "Comentario y calificación del resumen",
    "thumbnail": {
      "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
    },
    "contentBlock": [
      {
        "mainContent": {
          "value": {
            "schema": "dast",
            "document": {
              "type": "root",
              "children": [
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Publicado el 19 de septiembre"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Autores:"
                    },
                    {
                      "url": "https://huggingface.co/ljtsuda",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "Juntao Li"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ","
                    },
                    {
                      "url": "https://huggingface.co/ZetangForward",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "Zecheng Tang"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ",Yuyang Ding,"
                    },
                    {
                      "url": "https://huggingface.co/PinzhengWang",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "Pinzheng Wang"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ",Pei Guo,"
                    },
                    {
                      "url": "https://huggingface.co/Moriarty0923",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "Wangjie You"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ","
                    },
                    {
                      "url": "https://huggingface.co/jorjordan",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "Dan Qiao"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ","
                    },
                    {
                      "url": "https://huggingface.co/jokephp",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "Wenliang Chen"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ",Guohong Fu,Qiaoming Zhu,Guodong Zhou,Min Zhang"
                    }
                  ]
                },
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "Resumen"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Los grandes modelos de lenguaje (LLM) con miles de millones de parámetros han demostrado un rendimiento sobresaliente en varias tareas de procesamiento de lenguaje natural. Este informe presenta OpenBA, un modelo secuencial asimétrico bilingüe de 15B de código abierto, para contribuir con una variante de LLM a la comunidad de modelos de código abierto orientada al chino. Mejoramos OpenBA con técnicas efectivas y eficientes, así como adoptamos una estrategia de entrenamiento en tres etapas para entrenar el modelo desde cero. Nuestra solución también puede lograr un rendimiento muy competitivo con solo 380B tokens, lo cual es mejor que LLaMA-70B en el benchmark BELEBELE, BLOOM-176B en el benchmark MMLU, GLM-130B en el benchmark C-Eval (difícil). Este informe proporciona los detalles principales para preentrenar un modelo análogo, incluyendo el procesamiento de datos de preentrenamiento, la recopilación de datos bilingües de Flan, las observaciones empíricas que inspiran nuestro diseño de arquitectura de modelo, los objetivos de entrenamiento de diferentes etapas y otras técnicas de mejora. Hemos reestructurado nuestro código para seguir los principios de diseño de la Biblioteca de Transformadores de Huggingface, haciéndolo más conveniente para los desarrolladores, y hemos publicado puntos de control de diferentes etapas de entrenamiento en https://huggingface.co/openBA. Más detalles de nuestro proyecto están disponibles en https://github.com/OpenNLG/openBA.git."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "url": "https://arxiv.org/abs/2309.10706",
                      "meta": [
                        {
                          "id": "rel",
                          "value": "noreferrer"
                        },
                        {
                          "id": "target",
                          "value": "_blank"
                        }
                      ],
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "Ver página de arXiv"
                        }
                      ]
                    },
                    {
                      "url": "https://arxiv.org/pdf/2309.10706",
                      "meta": [
                        {
                          "id": "rel",
                          "value": "noreferrer"
                        },
                        {
                          "id": "target",
                          "value": "_blank"
                        }
                      ],
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "Ver PDF"
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "Comentario"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "El artículo \"OpenBA: Un modelo secuencial asimétrico bilingüe de 15B de código abierto preentrenado desde cero\" introduce un gran modelo de lenguaje bilingüe adaptado para aplicaciones orientadas al chino."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Puntos clave:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "numbered",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                {
                                  "0": "s",
                                  "1": "t",
                                  "2": "r",
                                  "3": "o",
                                  "4": "n",
                                  "5": "g"
                                }
                              ],
                              "value": "Modelo bilingüe"
                            },
                            {
                              "type": "span",
                              "value": ": OpenBA es un modelo bilingüe que atiende tareas orientadas al chino, llenando así un vacío en la comunidad de LLM."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                {
                                  "0": "s",
                                  "1": "t",
                                  "2": "r",
                                  "3": "o",
                                  "4": "n",
                                  "5": "g"
                                }
                              ],
                              "value": "Técnicas eficientes y estrategia de entrenamiento"
                            },
                            {
                              "type": "span",
                              "value": ": Los autores utilizan técnicas eficientes y una estrategia de entrenamiento en tres etapas para entrenar el modelo desde cero. Esto asegura que el modelo sea competitivo a pesar de ser entrenado con menos tokens en comparación con otros modelos grandes."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                {
                                  "0": "s",
                                  "1": "t",
                                  "2": "r",
                                  "3": "o",
                                  "4": "n",
                                  "5": "g"
                                }
                              ],
                              "value": "Rendimiento competitivo"
                            },
                            {
                              "type": "span",
                              "value": ": OpenBA logra un mejor rendimiento en múltiples benchmarks en comparación con otros modelos de última generación, con menos tokens. Esto sugiere una arquitectura y estrategias de entrenamiento eficientes."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                {
                                  "0": "s",
                                  "1": "t",
                                  "2": "r",
                                  "3": "o",
                                  "4": "n",
                                  "5": "g"
                                }
                              ],
                              "value": "Código abierto"
                            },
                            {
                              "type": "span",
                              "value": ": Los autores han hecho su modelo de código abierto e integrado con la Biblioteca de Transformadores de Huggingface. Esto facilita una adopción más fácil por parte de desarrolladores e investigadores."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Impacto potencial en el mundo real:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                {
                                  "0": "s",
                                  "1": "t",
                                  "2": "r",
                                  "3": "o",
                                  "4": "n",
                                  "5": "g"
                                }
                              ],
                              "value": "Tareas bilingües"
                            },
                            {
                              "type": "span",
                              "value": ": La naturaleza bilingüe de OpenBA puede abordar una amplia gama de tareas que involucran los idiomas inglés y chino, expandiendo la aplicabilidad de los LLM en regiones de habla china y aplicaciones bilingües."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                {
                                  "0": "s",
                                  "1": "t",
                                  "2": "r",
                                  "3": "o",
                                  "4": "n",
                                  "5": "g"
                                }
                              ],
                              "value": "Promoción de la investigación orientada al chino"
                            },
                            {
                              "type": "span",
                              "value": ": El enfoque del modelo en tareas orientadas al chino puede fomentar más investigación y aplicaciones que atiendan a este importante grupo lingüístico."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                {
                                  "0": "s",
                                  "1": "t",
                                  "2": "r",
                                  "3": "o",
                                  "4": "n",
                                  "5": "g"
                                }
                              ],
                              "value": "Herramienta accesible"
                            },
                            {
                              "type": "span",
                              "value": ": Con el modelo integrado en Huggingface y el código asociado siendo de código abierto, los desarrolladores e investigadores pueden adoptar, modificar y extender fácilmente este modelo para diversas aplicaciones."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                {
                                  "0": "s",
                                  "1": "t",
                                  "2": "r",
                                  "3": "o",
                                  "4": "n",
                                  "5": "g"
                                }
                              ],
                              "value": "Rendimiento en benchmarks"
                            },
                            {
                              "type": "span",
                              "value": ": El rendimiento superior en benchmarks sugiere la posibilidad de que este modelo se convierta en un estándar o referencia en tareas de PLN bilingües que involucren el chino."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Desafíos:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                {
                                  "0": "s",
                                  "1": "t",
                                  "2": "r",
                                  "3": "o",
                                  "4": "n",
                                  "5": "g"
                                }
                              ],
                              "value": "Naturaleza especializada"
                            },
                            {
                              "type": "span",
                              "value": ": Aunque el modelo es poderoso para tareas bilingües que involucran el chino, su especialización podría limitar su aplicabilidad más amplia en otros idiomas."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                {
                                  "0": "s",
                                  "1": "t",
                                  "2": "r",
                                  "3": "o",
                                  "4": "n",
                                  "5": "g"
                                }
                              ],
                              "value": "Intensidad de recursos"
                            },
                            {
                              "type": "span",
                              "value": ": Al igual que con otros modelos grandes, las aplicaciones en tiempo real o los despliegues en entornos con recursos limitados podrían enfrentar desafíos."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Dado el potencial de avances en tareas bilingües que involucren el chino y su contribución a la comunidad de código abierto:"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Calificaría el impacto en el mundo real de este artículo como un 8 de 10."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "OpenBA llena un nicho específico en el mundo de los LLM al atender tareas bilingües que involucran el chino. La naturaleza de código abierto y la integración con plataformas populares probablemente promoverán su adopción y estimularán más investigación en la comunidad de PLN en chino."
                    }
                  ]
                }
              ]
            }
          }
        },
        "topImages": [
          {
            "basename": "researchpaper9",
            "height": 816,
            "width": 1456,
            "filename": "researchpaper9.png",
            "format": "png",
            "alt": null,
            "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
          }
        ]
      }
    ],
    "seo": {
      "description": "Comentario y calificación del resumen\n",
      "title": "OpenBA: Un modelo ... bilingüe de 15B de código abierto ...",
      "twitterCard": null,
      "image": {
        "width": 1456,
        "height": 816,
        "title": null,
        "alt": null,
        "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
      }
    }
  },
  "topics": [
    "LLM",
    "Multilingual"
  ]
}