{
  "relatedBlogs": [
    {
      "id": "190259319",
      "topics": [
        "总结",
        "LLM",
        "训练"
      ],
      "title": "潜在空间播客 8/16/23 [总结] - 训练LLM的数学 — 与Eleuther AI的Quentin Anthony",
      "slug": "latent-space-podcast-8-16-23-summary-the-mathematics-of-training-llms-with-que",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T09:19:45+01:00",
      "description": "与Eleuther AI的Quentin Anthony一起探索训练LLM背后的数学。深入了解《Transformers Math 101》文章并掌握分布式训练技术以实现GPU性能的峰值。",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692324088-screenshot-2023-08-17-at-9-59-17-pm.png"
      },
      "seo": {
        "title": "潜在空间播客 8/16/23 [总结] 训练LLM的数学",
        "description": "深入了解《Transformers Math 101》文章并掌握分布式训练技术以实现GPU性能的峰值。"
      }
    },
    {
      "id": "190259087",
      "topics": [
        "总结",
        "LLM",
        "代码",
        "开源",
        "小模型"
      ],
      "title": "潜在空间播客 8/4/23 [总结] 潜在空间 x AI Breakdown 跨界播客！",
      "slug": "latent-space-podcast-8-4-23-summary-latent-space-x-ai-breakdown-crossover-pod",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T09:16:33+01:00",
      "description": "加入AI Breakdown和潜在空间的夏季AI技术综述：深入了解GPT4.5、Llama 2、AI工具、崛起的AI工程师等！",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1691539617-screenshot-2023-08-08-at-8-02-52-pm.png"
      },
      "seo": {
        "title": "潜在空间播客 8/4/23 [总结] AI Breakdown 跨界",
        "description": "深入了解GPT4.5、Llama 2、AI工具、崛起的AI工程师等！"
      }
    },
    {
      "id": "190259111",
      "topics": [
        "总结",
        "Transformers",
        "训练",
        "开源"
      ],
      "title": "潜在空间播客 7/26/23 [总结] FlashAttention 2：让Transformers快800% - Together AI的Tri Dao",
      "slug": "latent-space-podcast-7-26-23-summary-flashattention-2-making-transformers-800-fas",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T09:14:13+01:00",
      "description": "与Tri Dao一起探索FlashAttention如何通过FlashAttention 2革命性地提高AI速度，深入了解斯坦福的Hazy Lab和未来的AI见解。",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1691543194-screenshot-2023-08-08-at-8-43-59-pm.png"
      },
      "seo": {
        "title": "潜在空间播客 7/26/23 [总结] FlashAttention 2",
        "description": "与Tri Dao一起探索FlashAttention如何通过FlashAttention 2革命性地提高AI速度。"
      }
    },
    {
      "id": "190259172",
      "topics": [
        "总结",
        "LLM",
        "开源",
        "小模型"
      ],
      "title": "潜在空间播客 7/19/23 [总结] - Llama 2：新的开放LLM SOTA（特邀Nathan Lambert、Matt Bornstein、Anton Troynikov、Russell Kaplan、Whole Mars Catalog等）",
      "slug": "latent-space-podcast-7-19-23-summary-llama-2-the-new-open-llm-sota-ft-nathan-lamb",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T09:12:38+01:00",
      "description": "与专家Nathan Lambert、Matt Bornstein等一起探索Llama 2，最新的AI突破。深入了解数据集、基准测试和AI预测。在这顶级播客中等待Llama的见解和戏剧！",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1691968295-screenshot-2023-08-13-at-7-11-06-pm.png"
      },
      "seo": {
        "title": "潜在空间播客 7/19/23 [总结] - Llama 2",
        "description": "深入了解数据集、基准测试和AI预测。在这顶级播客中等待Llama的见解和戏剧！"
      }
    },
    {
      "id": "190259191",
      "topics": [
        "总结",
        "代码",
        "LLM"
      ],
      "title": "潜在空间播客 7/10/23 [总结] - 代码解释器 == GPT 4.5（与Simon Willison、Alex Volkov、Aravind Srinivas、Alex Graveley等）",
      "slug": "latent-space-podcast-7-10-23-summary-code-interpreter-gpt-4-5-w-simon-willison-al",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T09:09:26+01:00",
      "description": "探索ChatGPT的代码解释器：AI的游戏规则改变者。与Simon、Alex和顶级AI专家一起深入了解其1000倍能力提升。#CodeAugmentedInference #GPT4_5",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692048911-screenshot-2023-08-14-at-3-34-05-pm.png"
      },
      "seo": {
        "title": "潜在空间播客 [总结] 代码解释器 = GPT 4.5",
        "description": "探索ChatGPT的代码解释器：AI的游戏规则改变者。与Simon、Alex和顶级AI专家一起深入了解其1000倍能力提升。"
      }
    },
    {
      "id": "190259216",
      "topics": [
        "总结",
        "开源"
      ],
      "title": "潜在空间播客 7/2/23 [总结] AI趋势：潜在空间 x Practical AI 跨界播客！",
      "slug": "latent-space-podcast-7-2-23-summary-ai-trends-a-latent-space-x-practical-ai-cross",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T09:08:36+01:00",
      "description": "探索Practical AI和潜在空间的融合，深入探讨2023年的顶级AI趋势，回顾突出的剧集，并分享在AI演变中导航的见解。",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692146916-screenshot-2023-08-15-at-5-20-38-pm.png"
      },
      "seo": {
        "title": "潜在空间播客 7/2/23 [总结] AI趋势",
        "description": "2023年的顶级AI趋势，回顾突出的剧集，并分享在AI演变中导航的见解。"
      }
    },
    {
      "id": "190259238",
      "topics": [
        "硬件",
        "LLM",
        "总结"
      ],
      "title": "潜在空间播客 6/20/23 [总结] - 商品化Petaflop — 与tiny corp的George Hotz",
      "slug": "latent-space-podcast-6-20-23-summary-commoditizing-the-petaflop-with-george-ho",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T09:07:15+01:00",
      "description": "tiny corp的George Hotz挑战Nvidia和Google！深入了解AMD合作、ggml、Mojo、Elon和GPT-4的见解，以及AI女友的窥探。",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692154615-screenshot-2023-08-15-at-10-55-40-pm.png"
      },
      "seo": {
        "title": "潜在空间播客 6/20/23 [总结] - George Hotz",
        "description": "tiny corp的George Hotz挑战Nvidia和Google！AMD合作、ggml、Mojo、Elon和GPT-4的见解，以及AI女友的窥探。"
      }
    },
    {
      "id": "190259294",
      "topics": [
        "LLM",
        "功能",
        "总结"
      ],
      "title": "潜在空间播客 6/14/23 [总结] - 紧急播客：OpenAI的新功能API，75%价格下降，4倍上下文长度（与Alex Volkov、Simon Willison、Riley Goodside、Joshua Lochner、Stefania Druga、Eric Elliott、Mayo Oshin等）",
      "slug": "latent-space-podcast-6-14-23-summary-emergency-pod-openai-s-new-functions-api-75",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T09:05:04+01:00",
      "description": "与Scale、Microsoft、Pinecone和Huggingface的顶级AI工程师一起探索2023年6月的OpenAI更新。深入了解代码与LLM范式并发现递归函数代理。",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692221668-screenshot-2023-08-16-at-5-32-29-pm.png"
      },
      "seo": {
        "title": "潜在空间播客 6/20/23 [总结] - 紧急播客",
        "description": "与Scale、Microsoft、Pinecone和Huggingface的顶级AI工程师一起探索2023年6月的OpenAI更新。"
      }
    },
    {
      "id": "190259333",
      "topics": [
        "LLM",
        "总结",
        "用户体验"
      ],
      "title": "潜在空间播客 6/8/23 [总结] - 从RLHF到RLHB：学习人类行为的案例 - 与Amplitude的Jeffrey Wang和Joe Reeve",
      "slug": "latent-space-podcast-6-8-23-summary-from-rlhf-to-rlhb-the-case-for-learning-from",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T09:02:33+01:00",
      "description": "与Jeffrey Wang和Joe Reeve一起在潜在空间直播中探索AI和分析。深入了解为什么AI重视分析以及第一方行为数据的力量。",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692386432-screenshot-2023-08-18-at-3-17-04-pm.png"
      },
      "seo": {
        "title": "潜在空间播客 6/8/23 [总结] - 从RLHF到RLHB",
        "description": "与Jeffrey Wang和Joe Reeve一起在潜在空间直播中探索AI和分析。深入了解为什么AI重视分析以及第一方行为数据的力量。"
      }
    },
    {
      "id": "190260528",
      "topics": [
        "总结",
        "LLM",
        "用户体验"
      ],
      "title": "潜在空间播客 6/1/23 [总结] - 构建AI × UX Scenius — 与Notion AI的Linus Lee",
      "slug": "latent-space-podcast-6-1-23-summary-building-the-ai-x-ux-scenius-with-linus-le",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T09:00:12+01:00",
      "description": "探索Notion AI对AI和用户体验的变革性方法。深入了解AI增强工作空间的未来，超越聊天界面的价值，以及有效知识工作的见解。包括AI×UX NYC聚会的回顾！",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692390655-screenshot-2023-08-18-at-4-28-51-pm.png"
      },
      "seo": {
        "title": "潜在空间播客 6/1/23 [总结] - AI × UX Scenius",
        "description": "探索Notion AI对AI和用户体验的变革性方法。"
      }
    },
    {
      "id": "190260557",
      "topics": [
        "总结",
        "代码",
        "LLM",
        "代理"
      ],
      "title": "潜在空间播客 5/25/23 [总结] - 使用AI代理调试互联网 – 与Codium AI和AutoGPT的Itamar Friedman",
      "slug": "latent-space-podcast-5-25-23-summary-debugging-the-internet-with-ai-agents-with",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T08:58:27+01:00",
      "description": "与Codium AI的Itamar Friedman一起探索AI的未来，讨论“极端DRY”代理、规范和测试的快速同步以及代码与测试之间的平衡。还有来自Toran的见解和AutoGPT路线图的独家预览！",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692397413-screenshot-2023-08-18-at-6-10-09-pm.png"
      },
      "seo": {
        "title": "潜在空间播客 5/25/23 [总结] 调试互联网",
        "description": "讨论“极端DRY”代理、规范和测试的快速同步以及代码与测试之间的平衡。"
      }
    },
    {
      "id": "190260577",
      "topics": [
        "LLM",
        "小模型"
      ],
      "title": "潜在空间播客 5/20/23 [总结] - MPT-7B和Context=Infinity的开始 — 与MosaicML的Jonathan Frankle和Abhinav Venigalla",
      "slug": "latent-space-podcast-5-20-23-summary-mpt-7b-and-the-beginning-of-context-infinity",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T08:57:33+01:00",
      "description": "深入了解MosaicML的9天、20万美元“llongboi”MPT-7B训练、数据准备见解以及开放AI模型的兴起，与专家Frankle和Venigalla一起。",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692409795-screenshot-2023-08-18-at-9-49-21-pm.png"
      },
      "seo": {
        "title": "潜在空间播客 6/25/23 [总结] MosaicML",
        "description": "深入了解MosaicML的9天、20万美元“llongboi”MPT-7B训练、数据准备见解以及开放AI模型的兴起。"
      }
    },
    {
      "id": "190260597",
      "topics": [
        "LLM",
        "结构化数据"
      ],
      "title": "潜在空间播客 5/15/23 [总结] - 保证LLM输出的质量和结构 - 与Guardrails AI的Shreya Rajpal",
      "slug": "latent-space-podcast-5-15-23-summary-guaranteed-quality-and-structure-in-llm-outp",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T08:56:26+01:00",
      "description": "与Guardrails AI的Shreya Rajpal一起探索第12集：深入了解验证LLM输出，通过重新提问循环优化答案，并为模型建立SLA。掌握AI质量保证的细微差别。",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692495732-screenshot-2023-08-19-at-9-38-27-pm.png"
      },
      "seo": {
        "title": "潜在空间播客 5/15/23 [总结] 质量LLM输出",
        "description": "与Guardrails AI的Shreya Rajpal一起探索第12集：深入了解验证LLM输出。"
      }
    },
    {
      "id": "190260606",
      "topics": [
        "LLM",
        "训练",
        "代理",
        "多模态"
      ],
      "title": "潜在空间播客 5/8/23 [总结] - AI创始基因：早期、快速构建和相信伟大 — 与Lexica的Sharif Shameem",
      "slug": "latent-space-podcast-5-8-23-summary-the-ai-founder-gene-being-early-building-fast",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T08:52:09+01:00",
      "description": "第11集与Lexica的Sharif Shameem：深入了解AI创始人的心态，揭示开创性创新的秘密，构建改变游戏规则的技术，训练模型，以及代理和基因组测序的有趣潜力。",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692501984-screenshot-2023-08-19-at-11-24-05-pm.png"
      },
      "seo": {
        "title": "潜在空间播客 5/8/23 [总结] AI创始基因",
        "description": "第11集与Lexica的Sharif Shameem：深入了解AI创始人的心态，揭示开创性创新的秘密。"
      }
    },
    {
      "id": "190260640",
      "topics": [
        "总结",
        "开源",
        "LLM"
      ],
      "title": "潜在空间播客 5/5/23 [总结] - 无护城河：封闭AI的开源觉醒 — 特邀Simon Willison",
      "slug": "latent-space-podcast-5-5-23-summary-no-moat-closed-ai-gets-its-open-source-wakeup",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T08:49:18+01:00",
      "description": "与Simon Willison一起探索“无护城河：封闭AI的开源觉醒”。深入了解泄露的Google Moat备忘录见解、Google Brain Drain和Mojo的Python速度提升。",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692566921-screenshot-2023-08-20-at-5-25-53-pm.png"
      },
      "seo": {
        "title": "潜在空间播客 5/5/23 [总结] - 无护城河",
        "description": "与Simon Willison一起探索“无护城河：封闭AI的开源觉醒”。深入了解泄露的Google Moat备忘录见解。"
      }
    },
    {
      "id": "190260671",
      "topics": [
        "LLM",
        "代码",
        "总结"
      ],
      "title": "潜在空间播客 5/3/23 [总结] - 在1周内训练SOTA代码LLM并量化氛围 — 与Replit的Reza Shabani",
      "slug": "latent-space-podcast-5-3-23-summary-training-a-sota-code-llm-in-1-week-and-quanti",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T08:46:08+01:00",
      "description": "第10集与Replit的Reza Shabani：深入了解快速训练的最先进代码LLM，探索Replit Ghostwriter的未来，从金融到AI的旅程。发现从Kaplan到Chinchilla的过渡等！",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1692584998-screenshot-2023-08-20-at-10-17-26-pm.png"
      },
      "seo": {
        "title": "潜在空间播客 5/3/23 [总结] - SOTA代码LLM",
        "description": "第10集与Replit的Reza Shabani：深入了解快速训练的最先进代码LLM！"
      }
    },
    {
      "id": "190629271",
      "topics": [
        "LLM",
        "小模型",
        "总结"
      ],
      "title": "潜在空间播客 4/28/23 [总结] - 绘制真正开放模型的未来并以30美元训练Dolly — 与Databricks的Mike Conover",
      "slug": "latent-space-podcast-4-28-23-summary-mapping-the-future-of-truly-open-models-and",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T08:33:12+01:00",
      "description": "与Databricks的Mike Conover一起探索开放模型的未来。深入了解Dolly的创建，从1.0到2.0的过渡及其开发背后的影响。第9集涉及模型基础设施、Databricks的愿景等。#AI #OpenModels #Dolly",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1694038707-screenshot-2023-09-06-at-3-12-24-pm.png"
      },
      "seo": {
        "title": "潜在空间播客 4/28/23 [总结] - Mike of Databricks",
        "description": "第9集涉及模型基础设施、Databricks的愿景等。#AI #OpenModels #Dolly"
      }
    },
    {
      "id": "191164291",
      "topics": [
        "LLM",
        "企业",
        "总结"
      ],
      "title": "潜在空间播客 4/21/23 [总结] - 企业的AI驱动搜索 — 与Glean的Deedy Das",
      "slug": "latent-space-podcast-4-21-23-summary-ai-powered-search-for-the-enterprise-with",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T08:31:31+01:00",
      "description": "第8集：与Glean的Deedy Das一起深入探讨企业搜索中的AI。解析创建AI搜索巨头的挑战，Google与ChatGPT的比较，AI基础设施的复杂性，识别AI生成的文本，以及为什么企业需要的不仅仅是文档QA。",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1694134074-screenshot-2023-09-07-at-5-43-48-pm.png"
      },
      "seo": {
        "title": "潜在空间播客 4/21/23 [总结] - 与Deedy Das",
        "description": "第8集：与Glean的Deedy Das一起深入探讨企业搜索中的AI。解析创建AI搜索巨头的挑战，Google与ChatGPT的比较。"
      }
    },
    {
      "id": "191165673",
      "topics": [
        "总结",
        "视觉"
      ],
      "title": "潜在空间播客 4/13/23 [总结] - Segment Anything模型和计算机视觉的难题 — 与Roboflow的Joseph Nelson",
      "slug": "latent-space-podcast-4-13-23-summary-segment-anything-model-and-the-hard-problems",
      "authorName": "Prof. Otto Nomos",
      "authorPicture": {
        "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
      },
      "_publishedAt": "2023-10-05T08:30:03+01:00",
      "description": "与Joseph Nelson一起探索第7集，讨论Meta的Segment Anything模型。深入了解计算机视觉的未来，OCR的重要性，图像分割等。#Roboflow #AI",
      "thumbnail": {
        "url": "https://www.datocms-assets.com/101962/1694150379-screenshot-2023-09-07-at-10-15-52-pm.png"
      },
      "seo": {
        "title": "潜在空间播客 4/13/23 [总结] - Segment Anything",
        "description": "深入了解计算机视觉的未来，OCR的重要性，图像分割等。#Roboflow #AI"
      }
    }
  ],
  "blogContent": {
    "id": "190259129",
    "topics": [
      "LLM",
      "硬件",
      "摘要",
      "边缘"
    ],
    "title": "潜在空间播客 2023年8月10日 [摘要]: LLM无处不在：使用MLC在浏览器和iPhone上运行70B模型——与CMU/OctoML的陈天奇",
    "slug": "latent-space-podcast-8-10-23-summary-llms-everywhere-running-70b-models-in-browse",
    "authorName": "Prof. Otto Nomos",
    "authorPicture": {
      "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
    },
    "_publishedAt": "2023-10-05T09:18:37+01:00",
    "description": "探索与陈天奇一起的MLC魔力：在浏览器和iPhone上部署70B模型。深入了解XGBoost、TVM的创建及通用AI部署的未来。",
    "thumbnail": {
      "url": "https://www.datocms-assets.com/101962/1691894611-screenshot-2023-08-12-at-10-42-43-pm.png"
    },
    "contentBlock": [
      {
        "mainContent": {
          "value": {
            "schema": "dast",
            "document": {
              "type": "root",
              "children": [
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "原始链接："
                    },
                    {
                      "url": "https://www.latent.space/p/llms-everywhere#details",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "LLM无处不在：使用MLC在浏览器和iPhone上运行70B模型——与CMU/OctoML的陈天奇"
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "摘要"
                    }
                  ]
                },
                {
                  "type": "heading",
                  "level": 3,
                  "children": [
                    {
                      "type": "span",
                      "value": "关于TQ和XGBoost"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "在最近一期的"
                    },
                    {
                      "type": "span",
                      "marks": [
                        "emphasis"
                      ],
                      "value": "潜在空间"
                    },
                    {
                      "type": "span",
                      "value": "播客中，主持人Alessio和Swyx与卡内基梅隆大学助理教授、机器学习社区的领军人物陈天奇（TQ）进行了对话。Tianqi身兼多职，包括与Catalyst Group和OctoML的合作，并在开源生态系统中留下了重要足迹，尤其是在Apache TVM、XGBoost和MXNet等项目中。"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "在坦诚的对话中，TQ分享了除了他的技术身份外，他还有一个独特的爱好，即在真实的素描本中绘制设计图，记录他在各种项目中的旅程。这些素描作为他软件项目的蓝图，并提供了他多年来思维过程的有形记录。"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Tianqi备受赞誉的项目XGBoost也被提及，突出了其起源和意外的成功。最初设计为深度学习模型兴起的替代品，XGBoost最终在树模型擅长的表格数据领域建立了自己的利基市场。讨论集中在树模型和深度学习的平衡与潜在融合上。TQ认为树模型的持久相关性，特别是考虑到它们的自然规则、可扩展性和互操作性。谈话以对未来的展望结束，暗示了变压器模型和树算法的融合以增强数据处理。"
                    }
                  ]
                }
              ]
            }
          }
        },
        "topImages": [
          {
            "basename": "screenshot-2023-08-12-at-10-42-43-pm",
            "height": 548,
            "width": 1538,
            "filename": "screenshot-2023-08-12-at-10-42-43-pm.png",
            "format": "png",
            "alt": null,
            "url": "https://www.datocms-assets.com/101962/1691894611-screenshot-2023-08-12-at-10-42-43-pm.png"
          }
        ]
      },
      {
        "mainContent": {
          "value": {
            "schema": "dast",
            "document": {
              "type": "root",
              "children": [
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "TVM编译器，MXNet"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Alessio提到了Tianqi开发的TVM编译器框架，该框架与ONNX几乎同时发布，寻求澄清它们的关系。Tianqi回顾了他在深度学习方面的历史，提到了他在使用卷积受限玻尔兹曼机进行ImageNet分类的工作，这在AlexNet出现之前。他分享了在手工制作NVIDIA CUDA内核时遇到的挑战，这花费了数月时间，结果发现模型并不高效。这次经历让他了解了在GPU上优化性能的复杂性。"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "在完成XGBoost的工作后，Tianqi与人合作开发了MXNet，这比CAFE和PyTorch等框架更早。认识到为不同硬件优化的困难，Tianqi寻求创建一个更自动化和通用的解决方案，从而开发了TVM。TVM编译器可以接收机器学习程序，应用优化技术，并生成与各种后端兼容的低级代码，包括NVIDIA和非NVIDIA平台。"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "虽然Tianqi从XGBoost转向TVM对Alessio来说似乎很重要，但Tianqi澄清说他的动机更多是享受编码过程和解决挑战。他认为自己是一个问题解决者，当面临挑战时，他会寻找工具或创建新工具来解决这些问题。他提到，这种方法与机器学习系统中考虑算法和系统优化的新兴趋势一致。"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "在讨论社区的成长时，Tianqi提到了MLsys，一个专注于机器学习系统的会议。Swyx提到Tianqi参与了ICML和NeurIPS等主要会议，暗示社区组织在他的工作中起作用，Tianqi对此表示肯定，指出这是他学术职责的一部分。"
                    }
                  ]
                }
              ]
            }
          }
        },
        "topImages": [
          {
            "basename": "abls3-img1",
            "height": 936,
            "width": 936,
            "filename": "abls3-img1.png",
            "format": "png",
            "alt": null,
            "url": "https://www.datocms-assets.com/101962/1691897144-abls3-img1.png"
          }
        ]
      },
      {
        "mainContent": {
          "value": {
            "schema": "dast",
            "document": {
              "type": "root",
              "children": [
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "MLsys，MCLLM和MLC"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "在Swyx、Tianqi和Alessio之间的对话中，讨论围绕MLsys、MLCLLM和机器学习编译（MLC）。以下是主要内容："
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "numbered",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "MLsys和MLCLLM"
                            },
                            {
                              "type": "span",
                              "value": ": Swyx提到Tianqi最近在MLsys中的冒险及其与MLCLLM在手机上的集成。他提到使用Llama 2和Vicuña，但寻求关于其他可用模型的澄清。"
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Tianqi的MLC之旅"
                            },
                            {
                              "type": "span",
                              "value": ": Tianqi解释了他进入MLC的过程，这是他们最初项目TVM的演变。主要目标是构建一个有效的机器学习编译器。从TVM获得的经验中，他们开始了第二次迭代，名为TVM Unity。MLCLLM本质上是一个MLC计划，旨在开发广泛应用的机器学习编译技术。一个成就是让机器学习模型在手机和其他通用平台上运行，包括苹果的M2 Mac。"
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "与PyTorch的集成"
                            },
                            {
                              "type": "span",
                              "value": ": 针对Swyx关于模型集成的查询，Tianqi强调虽然许多模型是在PyTorch中构建的，但目标是将它们引入TVM的程序表示形式TVM脚本。目标是优化各种平台上的模型，并确保它们的可移植性和效率。"
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "MLC作为一门学科"
                            },
                            {
                              "type": "span",
                              "value": ": Swyx指出，虽然许多人专注于编译器，但Tianqi在MLC中的细分领域似乎很有创新性。Tianqi认为机器学习编译将作为一个领域发展，从现有的编译器优化中汲取灵感，并结合机器学习和系统的知识。"
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "优化和库"
                            },
                            {
                              "type": "span",
                              "value": ": 在讨论依赖现有库进行优化的局限性时，Tianqi详细介绍了TVM的方法，该方法结合了使用现有库和自动生成库。这种方法有助于支持不太受支持的硬件。"
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "核心优化技术"
                            },
                            {
                              "type": "span",
                              "value": ": Tianqi提到了四种基本的优化技术：内核融合（智能地组合操作）、内存规划（高效分配内存）、循环变换（确保代码高效运行）和权重量化（减少内存使用）。他解释说，这些方法允许在各种平台上高效且可移植地运行机器学习模型。"
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "本质上，讨论强调了MLC的重要性以及平台和优化技术的演变，以使机器学习模型更具普遍适用性和效率。"
                    }
                  ]
                }
              ]
            }
          }
        },
        "topImages": [
          {
            "basename": "abls3-img2",
            "height": 936,
            "width": 936,
            "filename": "abls3-img2.png",
            "format": "png",
            "alt": null,
            "url": "https://www.datocms-assets.com/101962/1691897165-abls3-img2.png"
          }
        ]
      },
      {
        "mainContent": {
          "value": {
            "schema": "dast",
            "document": {
              "type": "root",
              "children": [
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "浏览器中的LLM"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "在与Swyx的讨论中，Tianqi阐明了像他这样的学者从仅发布见解转向构建实际产品（如开源项目和应用程序）的新兴趋势。Tianqi认为这种实践方法使学者能够直接面对现实问题，并确保他们的研究为公众提供即时价值。在他的领域，机器学习系统，Tianqi看到了将这些系统部署到用户手中的潜力，以推动创新并解决实际问题。"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "他详细介绍了在浏览器中运行一个700亿参数模型的经验，特别强调了执行这一壮举的挑战和要求。使用最新的配备M2 Max和WebGPU技术的MacBook，Tianqi的团队成功运行了该模型，展示了在无需安装的情况下在消费设备上操作强大模型的可能性。他设想了多种应用场景，包括在边缘和服务器组件上同时运行的混合模型。"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Alessio询问了浏览器模型集成，Tianqi介绍了一个NPM包，WebILM，允许开发者将他们的模型嵌入到Web应用中。此外，还在开发一个兼容OpenAI的REST API，以进一步简化集成。"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "最后，Swyx提到了模型下载的挑战，Tianqi提到Chrome缓存系统，它可以防止类似Web应用的冗余下载。当被问及本地模型项目的激增时，Tianqi强调了增强API功能的重要性，并鼓励一个专注于通用部署的协作生态系统。"
                    }
                  ]
                }
              ]
            }
          }
        },
        "topImages": [
          {
            "basename": "abls3-img3",
            "height": 936,
            "width": 936,
            "filename": "abls3-img3.png",
            "format": "png",
            "alt": null,
            "url": "https://www.datocms-assets.com/101962/1691897179-abls3-img3.png"
          }
        ]
      },
      {
        "mainContent": {
          "value": {
            "schema": "dast",
            "document": {
              "type": "root",
              "children": [
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "Octomel和结论"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Alessio通过讨论Tianqi作为Octomel联合创始人的角色及其最近发布的OctoAI（一种专注于模型运行时优化的计算服务）开始了对话。他询问了Octomel从传统MLOps工具到当前与OctoAI的演变，特别是在市场向预训练生成模型转变的背景下。"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Tianqi解释说，他们发现了与可扩展性、集成和优化相关的挑战。随着市场向生成AI转变，OctoAI旨在简化流程并减轻用户的复杂性，使他们能够专注于他们的模型，而Octomel处理底层基础设施。"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Alessio指出，市场上的一个重要瓶颈是AI模型的执行。早期，挑战在于由于缺乏人才而构建模型，但现在随着众多模型的可用，挑战在于高效地运行它们。"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Tianqi强调了与“运行”AI模型相关的细微差别。鉴于硬件可用性的多样性和不断变化的用户请求，执行挑战倍增。高效管理模型位置并确保与执行环境的接近性至关重要。未来，Tianqi认为需要利用所有可用硬件以降低成本并优化边缘设备和云之间的相互作用。"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "当Alessio探讨从最终用户抽象硬件细节的挑战时，Tianqi强调了与各种硬件兼容的重要性，并根据用户需求和反馈不断迭代产品。"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Swyx将对话引向更广泛的AI领域，Tianqi分享了他对开源项目的热情，特别是那些支持模型间交互的项目，以及多样化的AI代理生态系统的前景。"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Swyx随后询问了可能继承变压器的架构。Tianqi提到了像RWKV这样的模型和与变压器集成的其他递归网络，强调了模型领域的持续增长。"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "在闪电回合中，Tianqi透露了对会话聊天机器人迅速出现的惊讶。当被问及AI中最有趣的未解决问题时，他表达了对AI的持续学习和终身学习的兴趣。"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "作为最后的收获，Tianqi鼓励听众在构建AI应用程序时采用整体方法。一个成功的AI系统需要算法、系统优化和数据策划的融合。"
                    }
                  ]
                }
              ]
            }
          }
        },
        "topImages": [
          {
            "basename": "abls3-img4",
            "height": 936,
            "width": 936,
            "filename": "abls3-img4.png",
            "format": "png",
            "alt": null,
            "url": "https://www.datocms-assets.com/101962/1691897193-abls3-img4.png"
          }
        ]
      }
    ],
    "seo": {
      "description": "探索在浏览器和iPhone上部署70B模型。深入了解XGBoost、TVM的创建及通用AI部署的未来。",
      "title": "潜在空间 2023年8月10日 [摘要]: LLM无处不在",
      "twitterCard": null,
      "image": {
        "width": 1538,
        "height": 548,
        "title": null,
        "alt": null,
        "url": "https://www.datocms-assets.com/101962/1691894611-screenshot-2023-08-12-at-10-42-43-pm.png"
      }
    }
  },
  "topics": [
    "LLM",
    "Hardware",
    "Summary",
    "Edge"
  ]
}