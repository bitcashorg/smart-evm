{
  "relatedBlogs": [],
  "blogContent": {
    "id": "198277360",
    "topics": [
      "LLM",
      "RLHF"
    ],
    "title": "通过优势模型和选择性复习稳定RLHF",
    "slug": "stabilizing-rlhf-through-advantage-model-and-selective-rehearsal",
    "authorName": "Prof. Otto Nomos",
    "authorPicture": {
      "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
    },
    "_publishedAt": "2024-05-24T05:22:15+01:00",
    "description": "摘要评论与评分",
    "thumbnail": {
      "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
    },
    "contentBlock": [
      {
        "mainContent": {
          "value": {
            "schema": "dast",
            "document": {
              "type": "root",
              "children": [
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "发表于9月18日"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "作者:"
                    },
                    {
                      "url": "https://huggingface.co/Baolin",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "彭宝林"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ","
                    },
                    {
                      "url": "https://huggingface.co/freesunshine0316",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "宋林峰"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ",田野,金立峰,米海涛,于东"
                    }
                  ]
                },
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "摘要"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "大型语言模型（LLMs）在自然语言处理方面取得了革命性进展，但使用RLHF使这些模型与人类价值观和偏好对齐仍然是一个重大挑战。这个挑战的特点是各种不稳定性，如奖励黑客和灾难性遗忘。在这份技术报告中，我们提出了两项创新来稳定RLHF训练：1）优势模型，直接建模优势分数，即与预期奖励相比的额外奖励，并调节任务间的分数分布以防止奖励黑客。2）选择性复习，通过战略性选择数据进行PPO训练和知识复习来缓解灾难性遗忘。我们在公共和专有数据集上的实验分析表明，所提出的方法不仅增加了RLHF训练的稳定性，还实现了更高的奖励分数和胜率。"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "url": "https://arxiv.org/abs/2309.10202",
                      "meta": [
                        {
                          "id": "rel",
                          "value": "noreferrer"
                        },
                        {
                          "id": "target",
                          "value": "_blank"
                        }
                      ],
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "View arXiv page"
                        }
                      ]
                    },
                    {
                      "url": "https://arxiv.org/pdf/2309.10202",
                      "meta": [
                        {
                          "id": "rel",
                          "value": "noreferrer"
                        },
                        {
                          "id": "target",
                          "value": "_blank"
                        }
                      ],
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "View PDF"
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "Commentary"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "The paper \"Stabilizing RLHF through Advantage Model and Selective Rehearsal\" delves into addressing the challenges that come with aligning Large Language Models (LLMs) with human values and preferences using Reinforcement Learning from Human Feedback (RLHF)."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Key Takeaways:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "numbered",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Challenges with RLHF"
                            },
                            {
                              "type": "span",
                              "value": ": Aligning LLMs to human preferences using RLHF presents hurdles like reward hacking (where the model finds ways to maximize reward without actually providing the intended value) and catastrophic forgetting (where a model forgets previously learned tasks when learning new ones)."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Advantage Model"
                            },
                            {
                              "type": "span",
                              "value": ": This technique aims to prevent reward hacking by modeling the advantage score, which is the extra reward compared to expected rewards, and regulating score distributions across tasks."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Selective Rehearsal"
                            },
                            {
                              "type": "span",
                              "value": ": To counteract catastrophic forgetting, this method strategically selects data for PPO training and knowledge rehearsing."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Positive Results"
                            },
                            {
                              "type": "span",
                              "value": ": The paper reports that the introduced methods not only enhance stability in RLHF training but also lead to higher reward scores and win rates."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Potential Real-World Impact:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Better Alignment with Human Values"
                            },
                            {
                              "type": "span",
                              "value": ": If LLMs can be better trained to align with human values using RLHF, the resultant models will produce outputs that are more desirable, safe, and user-centric."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Robust LLMs"
                            },
                            {
                              "type": "span",
                              "value": ": The proposed techniques could lead to models that are less susceptible to potential pitfalls, making them more reliable for critical tasks."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Broad Applicability"
                            },
                            {
                              "type": "span",
                              "value": ": While the focus is on LLMs, the techniques presented could have broader implications for other machine learning models where alignment with human feedback is crucial."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Industry Standard"
                            },
                            {
                              "type": "span",
                              "value": ": If the introduced methods are robust and effective, they might become standard techniques in RLHF for LLMs, leading to a widespread impact on how models are trained in the future."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Challenges:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Implementation"
                            },
                            {
                              "type": "span",
                              "value": ": Despite the reported advantages, the real-world impact depends on how easily these techniques can be implemented in various scenarios and how they interact with other techniques and methods."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Given the paper's focus on stabilizing RLHF, a crucial aspect of training LLMs, and the promising results they report:"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "I'd rate the real-world impact of this paper as a 9 out of 10."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "The stabilization of RLHF training is pivotal in ensuring LLMs align well with human values. Implementing these techniques could lead to safer and more reliable language models, which in turn would benefit a wide array of applications across industries."
                    }
                  ]
                }
              ]
            }
          }
        },
        "topImages": [
          {
            "basename": "researchpaper9",
            "height": 816,
            "width": 1456,
            "filename": "researchpaper9.png",
            "format": "png",
            "alt": null,
            "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
          }
        ]
      }
    ],
    "seo": {
      "description": "摘要评论与评分\n",
      "title": "通过优势模型和选择性复习稳定RLHF",
      "twitterCard": null,
      "image": {
        "width": 1456,
        "height": 816,
        "title": null,
        "alt": null,
        "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
      }
    }
  },
  "topics": [
    "LLM",
    "RLHF"
  ]
}