{
  "relatedBlogs": [],
  "blogContent": {
    "id": "198277345",
    "topics": [
      "LLM",
      "多语言"
    ],
    "title": "百川2：开放的大规模语言模型",
    "slug": "baichuan-2-open-large-scale-language-models",
    "authorName": "Prof. Otto Nomos",
    "authorPicture": {
      "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
    },
    "_publishedAt": "2024-05-24T05:59:10+01:00",
    "description": "摘要评论与评分",
    "thumbnail": {
      "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
    },
    "contentBlock": [
      {
        "mainContent": {
          "value": {
            "schema": "dast",
            "document": {
              "type": "root",
              "children": [
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "发表于9月18日"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "作者：杨爱元,"
                    },
                    {
                      "url": "https://huggingface.co/BinXiao",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "肖斌"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ",王炳宁,张博荣,尹超,吕晨旭,潘达,"
                    },
                    {
                      "url": "https://huggingface.co/wangdianhellen",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "王典"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ",严东,杨帆,邓飞,王峰,刘峰,艾广伟,董国胜,赵海洲,徐航,孙浩泽,"
                    },
                    {
                      "url": "https://huggingface.co/hongdaaaaaaaa",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "张宏达"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ",刘辉,"
                    },
                    {
                      "url": "https://huggingface.co/jijiaming",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "季佳明"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ","
                    },
                    {
                      "url": "https://huggingface.co/hsaest",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "谢健"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ","
                    },
                    {
                      "url": "https://huggingface.co/calico-1226",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "戴俊涛"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": "+30位作者"
                    }
                  ]
                },
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "摘要"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "大语言模型（LLM）在仅基于少量自然语言指令示例的各种自然语言任务中表现出色，减少了对广泛特征工程的需求。然而，大多数强大的LLM是闭源的或在非英语语言方面能力有限。在这份技术报告中，我们介绍了百川2，一系列包含70亿和130亿参数的大规模多语言模型，从头开始训练，使用了2.6万亿个标记。百川2在MMLU、CMMLU、GSM8K和HumanEval等公共基准测试中匹敌或超越了其他同等规模的开源模型。此外，百川2在医学和法律等垂直领域表现出色。我们将发布所有预训练模型检查点，以帮助研究社区更好地理解百川2的训练动态。"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "url": "https://arxiv.org/abs/2309.10305",
                      "meta": [
                        {
                          "id": "rel",
                          "value": "noreferrer"
                        },
                        {
                          "id": "target",
                          "value": "_blank"
                        }
                      ],
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "View arXiv page"
                        }
                      ]
                    },
                    {
                      "url": "https://arxiv.org/pdf/2309.10305",
                      "meta": [
                        {
                          "id": "rel",
                          "value": "noreferrer"
                        },
                        {
                          "id": "target",
                          "value": "_blank"
                        }
                      ],
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "View PDF"
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "Commentary"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "The paper \"Baichuan 2: Open Large-scale Language Models\" introduces a series of large-scale multilingual language models and emphasizes their capabilities across various tasks, including domain-specific applications like medicine and law."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Key Takeaways:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "numbered",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Multilingual LLM"
                            },
                            {
                              "type": "span",
                              "value": ": Baichuan 2 is multilingual, making it suitable for tasks across multiple languages, addressing the limitation of other powerful LLMs that focus primarily on English."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Significant Scale"
                            },
                            {
                              "type": "span",
                              "value": ": The model boasts 7 billion and 13 billion parameters and was trained on a massive 2.6 trillion tokens, making it a powerful LLM."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Benchmark Performance"
                            },
                            {
                              "type": "span",
                              "value": ": Baichuan 2 performs competitively on public benchmarks, matching or even surpassing other open-source models of similar size."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Domain Specialization"
                            },
                            {
                              "type": "span",
                              "value": ": The model showcases excellence in vertical domains such as medicine and law, indicating its versatility."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Open-Source Availability"
                            },
                            {
                              "type": "span",
                              "value": ": All pre-training model checkpoints will be released, aiding the research community in understanding the training dynamics of Baichuan 2."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Potential Real-World Impact:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Wide Applicability"
                            },
                            {
                              "type": "span",
                              "value": ": The multilingual nature of Baichuan 2 allows it to be applied to various tasks across different languages, making it a versatile tool in the global NLP ecosystem."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "High-Value Domains"
                            },
                            {
                              "type": "span",
                              "value": ": The model's excellence in domains like medicine and law can pave the way for domain-specific applications such as legal document parsing or medical diagnosis assistance based on textual data."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Research Impetus"
                            },
                            {
                              "type": "span",
                              "value": ": The open-source nature of the model will likely encourage more research into understanding and improving large-scale LLMs, pushing the boundaries of what they can achieve."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Reduced Feature Engineering"
                            },
                            {
                              "type": "span",
                              "value": ": Given its performance with minimal examples, Baichuan 2 can significantly reduce the need for feature engineering in NLP tasks, simplifying model development processes."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Challenges:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Resource Intensiveness"
                            },
                            {
                              "type": "span",
                              "value": ": Such large models often come with high computational costs, making their real-time deployment in certain environments challenging."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Potential Biases"
                            },
                            {
                              "type": "span",
                              "value": ": Like other LLMs, the risk of biases inherent in the training data might manifest in the model's outputs, especially given its scale."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Given the model's significant scale, multilingual capabilities, high performance across benchmarks, and domain-specific excellence:"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "I'd rate the real-world impact of this paper as a 9 out of 10."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Baichuan 2 addresses a critical gap in the LLM space by providing a powerful multilingual model. Its competitive performance, combined with the potential for domain-specific applications, makes it an impactful contribution to the field of NLP."
                    }
                  ]
                }
              ]
            }
          }
        },
        "topImages": [
          {
            "basename": "researchpaper9",
            "height": 816,
            "width": 1456,
            "filename": "researchpaper9.png",
            "format": "png",
            "alt": null,
            "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
          }
        ]
      }
    ],
    "seo": {
      "description": "摘要评论与评分\n",
      "title": "百川2：开放的大规模语言模型",
      "twitterCard": null,
      "image": {
        "width": 1456,
        "height": 816,
        "title": null,
        "alt": null,
        "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
      }
    }
  },
  "topics": [
    "LLM",
    "Multilingual"
  ]
}