{
  "relatedBlogs": [],
  "blogContent": {
    "id": "198277342",
    "topics": [
      "LLM",
      "多语言"
    ],
    "title": "OpenBA：一个从头开始预训练的开源15B双语不对称seq2seq模型",
    "slug": "openba-an-open-sourced-15b-bilingual-asymmetric-seq2seq-model-pre-trained-from-sc",
    "authorName": "Prof. Otto Nomos",
    "authorPicture": {
      "url": "https://www.datocms-assets.com/101962/1692842125-profottonomosheadshot.png"
    },
    "_publishedAt": "2024-05-27T03:34:02+01:00",
    "description": "摘要评论与评分",
    "thumbnail": {
      "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
    },
    "contentBlock": [
      {
        "mainContent": {
          "value": {
            "schema": "dast",
            "document": {
              "type": "root",
              "children": [
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "发表于 9月19日"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "作者："
                    },
                    {
                      "url": "https://huggingface.co/ljtsuda",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "李俊涛"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ","
                    },
                    {
                      "url": "https://huggingface.co/ZetangForward",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "唐泽成"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ",丁宇扬,"
                    },
                    {
                      "url": "https://huggingface.co/PinzhengWang",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "王品正"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ",郭培,"
                    },
                    {
                      "url": "https://huggingface.co/Moriarty0923",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "尤望杰"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ","
                    },
                    {
                      "url": "https://huggingface.co/jorjordan",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "乔丹"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ","
                    },
                    {
                      "url": "https://huggingface.co/jokephp",
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "陈文亮"
                        }
                      ]
                    },
                    {
                      "type": "span",
                      "value": ",傅国宏,朱乔明,周国栋,张敏"
                    }
                  ]
                },
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "摘要"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "具有数十亿参数的大型语言模型（LLM）在各种自然语言处理任务中表现出色。本文介绍了OpenBA，一个开源的15B双语不对称seq2seq模型，为以中文为主的开源模型社区贡献了一种LLM变体。我们通过有效和高效的技术增强了OpenBA，并采用了三阶段训练策略从头开始训练模型。我们的解决方案仅使用380B个标记就能实现非常有竞争力的性能，比BELEBELE基准上的LLaMA-70B、MMLU基准上的BLOOM-176B、C-Eval（难）基准上的GLM-130B表现更好。本文提供了预训练类似模型的主要细节，包括预训练数据处理、双语Flan数据收集、启发我们模型架构设计的经验观察、不同阶段的训练目标和其他增强技术。我们已重构代码以遵循Huggingface Transformers库的设计原则，使开发者更方便使用，并在https://huggingface.co/openBA发布了不同训练阶段的检查点。更多项目详情请访问https://github.com/OpenNLG/openBA.git。"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "url": "https://arxiv.org/abs/2309.10706",
                      "meta": [
                        {
                          "id": "rel",
                          "value": "noreferrer"
                        },
                        {
                          "id": "target",
                          "value": "_blank"
                        }
                      ],
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "View arXiv page"
                        }
                      ]
                    },
                    {
                      "url": "https://arxiv.org/pdf/2309.10706",
                      "meta": [
                        {
                          "id": "rel",
                          "value": "noreferrer"
                        },
                        {
                          "id": "target",
                          "value": "_blank"
                        }
                      ],
                      "type": "link",
                      "children": [
                        {
                          "type": "span",
                          "value": "View PDF"
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "heading",
                  "level": 2,
                  "children": [
                    {
                      "type": "span",
                      "value": "Commentary"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "The paper \"OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model Pre-trained from Scratch\" introduces a bilingual large language model tailored for Chinese-oriented applications."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Key Takeaways:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "numbered",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Bilingual Model"
                            },
                            {
                              "type": "span",
                              "value": ": OpenBA is a bilingual model catering to Chinese-oriented tasks, thereby filling a gap in the LLM community."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Efficient Techniques and Training Strategy"
                            },
                            {
                              "type": "span",
                              "value": ": The authors use efficient techniques and a three-stage training strategy to train the model from scratch. This ensures the model is competitive despite being trained on fewer tokens compared to other large models."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Competitive Performance"
                            },
                            {
                              "type": "span",
                              "value": ": The OpenBA achieves better performance on multiple benchmarks compared to other state-of-the-art models, with fewer tokens. This suggests efficient architecture and training strategies."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Open Source"
                            },
                            {
                              "type": "span",
                              "value": ": The authors have made their model open source and integrated it with the Huggingface Transformers Library. This facilitates easier adoption by developers and researchers."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Potential Real-World Impact:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Bilingual Tasks"
                            },
                            {
                              "type": "span",
                              "value": ": OpenBA's bilingual nature can address a wide range of tasks that involve English and Chinese languages, expanding the applicability of LLMs in Chinese-speaking regions and bilingual applications."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Promotion of Chinese-Oriented Research"
                            },
                            {
                              "type": "span",
                              "value": ": The model's focus on Chinese-oriented tasks can encourage more research and applications catering to this significant language group."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Accessible Tool"
                            },
                            {
                              "type": "span",
                              "value": ": With the model being integrated into Huggingface and the associated code being open-sourced, developers and researchers can easily adopt, modify, and extend this model for various applications."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Benchmark Performance"
                            },
                            {
                              "type": "span",
                              "value": ": The superior performance on benchmarks hints at the possibility of this model becoming a standard or reference in bilingual NLP tasks involving Chinese."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Challenges:"
                    }
                  ]
                },
                {
                  "type": "list",
                  "style": "bulleted",
                  "children": [
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Specialized Nature"
                            },
                            {
                              "type": "span",
                              "value": ": While the model is powerful for bilingual tasks involving Chinese, its specialization might limit its broader applicability across other languages."
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "type": "listItem",
                      "children": [
                        {
                          "type": "paragraph",
                          "children": [
                            {
                              "type": "span",
                              "marks": [
                                "strong"
                              ],
                              "value": "Resource Intensiveness"
                            },
                            {
                              "type": "span",
                              "value": ": As with other large models, real-time applications or deployments in resource-constrained environments might face challenges."
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "Given the potential for breakthroughs in bilingual tasks involving Chinese and its contribution to the open-source community:"
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "I'd rate the real-world impact of this paper as an 8 out of 10."
                    }
                  ]
                },
                {
                  "type": "paragraph",
                  "children": [
                    {
                      "type": "span",
                      "value": "OpenBA fills a specific niche in the LLM world by catering to bilingual tasks involving Chinese. The open-source nature and integration with popular platforms will likely promote its adoption and stimulate further research in the Chinese NLP community."
                    }
                  ]
                }
              ]
            }
          }
        },
        "topImages": [
          {
            "basename": "researchpaper9",
            "height": 816,
            "width": 1456,
            "filename": "researchpaper9.png",
            "format": "png",
            "alt": null,
            "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
          }
        ]
      }
    ],
    "seo": {
      "description": "摘要评论与评分\n",
      "title": "OpenBA：一个开源的15B双语...模型...",
      "twitterCard": null,
      "image": {
        "width": 1456,
        "height": 816,
        "title": null,
        "alt": null,
        "url": "https://www.datocms-assets.com/101962/1692843326-researchpaper9.png"
      }
    }
  },
  "topics": [
    "LLM",
    "Multilingual"
  ]
}